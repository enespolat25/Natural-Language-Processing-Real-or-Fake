{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "News.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mertyyanik/Natural-Language-Processing-Real-or-Fake/blob/master/News.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "N0MgjiC5sUzr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Doğal Dil İşleme - Gerçek ya da Sahte?\n",
        "Doğal Dil işleme (Natural Language Processing), basit hali ile yapay zeka ve dil biliminin birleşmesi ile ortaya çıkmış bir kavramdır. Bu kavram herkesin günlük konuşma dilinin bilgisayara öğretilmesi ve bazı çalışmaların bilgisayara yaptırılmasına sebep olmuştur. Günümüze yakın tarihlere baktığımız zaman bilgisayarların güçlenmesini takip eden makine öğrenmesi modelleri daha da gelişmiş ve bir çok alanda kullanılmaya başlanmıştır. Bu alanlardan biriside doğal dil işlemedir. Doğal dil işlemeyi makine öğrenmesi modelleri ile birleştirdiğimiz zaman ortaya bir cümlenin kime ait olduğunu ya da bir cümleyi söyleyen kişinin nasıl bir ruh haline sahip olduğunu anlayabilir noktaya gelmiş bulunmaktayız.\n",
        "\n",
        "Bizimde amacımız buna benzer bir çalışma yapmaktır. Elimizde haber sitelerinin yazmış olduğu haber başlıkları ve bu başlıkların gerçek ya da sahte olduğunu gösteren bir veri kümesi bulunmaktadır. Derin öğrenme yöntemlerini kullanarak bu veri kümesini bilgisayara öğretecek ve gelecekte yazılmış bir haber başlığının gerçek mi yoksa sahte mi olduğunu anlamaya çalışacağız."
      ]
    },
    {
      "metadata": {
        "id": "FIkAaBFktE21",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Bunun için Google'ın geliştirmiş olduğu Colab ortamını kullanıyoruz. Bu ortamın avantajı derin öğrenme yöntemlerinde uygulamaları eğitirken **TPU**'ları kullanıyor olmasıdır. TPU'lar, GPU veya CPU'ya göre 15 kat daha hızlıdır. Bu da bizi büyük bir oranda zaman maliyetinden kurtarmaktadır."
      ]
    },
    {
      "metadata": {
        "id": "Wb9SkSN9S5UF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Öncelikle elimizde ki veri kümelerini Colab ortamına aktarmamız gerekiyor. Bunu yapmadan önce ortamın Google Drive ile olan bağlantısını onaylamamız lazım. Aşağıda ki kod bloğunu çalıştırdığımız zaman ekranda bir link belirmektedir bu linke tıklayarak karşımıza çıkan kodu output kısmına yapıştırarak Google Drive ile olan bağlantıyı sağlamış oluyoruz."
      ]
    },
    {
      "metadata": {
        "id": "29Lt9X4PbrWI",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        " \n",
        "from pydrive.auth import GoogleAuth\n",
        " \n",
        "from pydrive.drive import GoogleDrive\n",
        " \n",
        "from google.colab import auth\n",
        " \n",
        "from oauth2client.client import GoogleCredentials\n",
        " \n",
        "auth.authenticate_user()\n",
        " \n",
        "gauth = GoogleAuth()\n",
        " \n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "itSyx3Sql4cA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Bundan sonraki aşama veri kümelerini Google Drive'dan indirmektir. Burada ki önemli nokta dosya numarasının (File ID) elde edilmesidir. Bu numaraya  Google Drive üzerinden veri kümesinin paylaş butonuna basarak ulaşabilirsiniz."
      ]
    },
    {
      "metadata": {
        "id": "Zb5eh3gMcXGh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Test.csv Dosyası\n",
        "from google.colab import files\n",
        "\n",
        "file_id = '1Zx7P0t9lQLP4aj4YOUaZDxNRrSkZOTk4'\n",
        "\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "\n",
        "downloaded.GetContentFile('test.csv')\n",
        "\n",
        "#Test.txt Dosyası\n",
        "from google.colab import files\n",
        " \n",
        "file_id = '1S2-N2HSIgtZpSzPmAEZWmVxV2YlOzCzs'\n",
        " \n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        " \n",
        "downloaded.GetContentFile('test.txt')\n",
        "\n",
        "#clean_real-Train.txt Dosyası\n",
        "from google.colab import files\n",
        " \n",
        "file_id = '1pnsf7izWzNYlA41xcWrCXQ5SSFe5JCUF'\n",
        " \n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        " \n",
        "downloaded.GetContentFile('clean_real-Train.txt')\n",
        "\n",
        "#clean_fake-Train.txt Dosyası\n",
        "from google.colab import files\n",
        " \n",
        "file_id = '1Mc1m6S6WCJuRkMBWM9_LZGHSzJIw2-e0'\n",
        " \n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        " \n",
        "downloaded.GetContentFile('clean_fake-Train.txt')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_Fe5yhr9m_eg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Bu noktadan sonra artık kendi Python kodlamanızı istediğiniz gibi yazabiliriz.\n",
        "\n",
        "Öncelikle gerekli kütüphaneleri yükleyelim."
      ]
    },
    {
      "metadata": {
        "id": "uyfureqInTdY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R0ZYcTFZMivK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Text dosyasında bulunan veri dosyalarımızı ayrı ayrı listelere aktarıyoruz. Böylece Python'ın nimetlerinden faydalanabileceğiz."
      ]
    },
    {
      "metadata": {
        "id": "y_8AGswcdRCv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Reading Data\n",
        "input1Data = open('clean_real-Train.txt', 'r+')\n",
        "input2Data = open('clean_fake-Train.txt', 'r+')\n",
        "testData = open('test.txt')\n",
        "\n",
        "testDataForComparison = pd.read_csv('test.csv').iloc[:, 1:]\n",
        "\n",
        "input1List = []\n",
        "input2List = []\n",
        "testList = []\n",
        "\n",
        "read = input1Data.readline()\n",
        "read2 = input2Data.readline()\n",
        "read3 = testData.readline()\n",
        "rowCount = 0\n",
        "while read != \"\":\n",
        "    input1List.append(read)\n",
        "    read = input1Data.readline()\n",
        "    \n",
        "while read2 != \"\":\n",
        "    input2List.append(read2)\n",
        "    read2 = input2Data.readline()\n",
        "\n",
        "while read3 != \"\":\n",
        "    testList.append(read3)\n",
        "    read3 = testData.readline()\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AM0hwXewMn0W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Daha sonra iki farklı veri kümesinde bulunan 'real' ve 'fake' veri kümelerini birleştiriyoruz. Real olanlara 1, fake olanlara 0 yazıyoruz."
      ]
    },
    {
      "metadata": {
        "id": "HtMraR2NhkjO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Real and Fake data combined.    \n",
        "liste = ['real' for i in range(len(input1List))]\n",
        "realInput = np.column_stack((input1List, liste))\n",
        "\n",
        "\n",
        "liste = ['fake' for i in range(len(input2List))]\n",
        "fakeInput = np.column_stack((input2List, liste))\n",
        "\n",
        "mainInput = realInput.tolist() + fakeInput.tolist()\n",
        "random.shuffle(mainInput)\n",
        "\n",
        "sentenceInput = []\n",
        "targetInput = []\n",
        "targetInputFloat = []\n",
        "for i in range(len(mainInput)):\n",
        "    sentenceInput.append(mainInput[i][0])\n",
        "    \n",
        "for i in range(len(mainInput)):\n",
        "    targetInput.append(mainInput[i][1])\n",
        "    \n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "lb = LabelEncoder()\n",
        "targetInputFloat = lb.fit_transform(targetInput)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3D3wvtrBNfFD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Veri Ön İşleme\n",
        "Bundan sonraki aşama veri ön işlemenin başladığı kısım. Veri kümesini bilgisayarın anlayabileceği sayılara çevirmeden önce bakıldığı zaman görülen bazı yanlışlar yok mu? Tabiki var. Bu yanlışların ne olduğundan bahsetmemiz ve en nihayetinde bunlardan kurtulmamız lazım. Yani veri kümemizi bazı veri ön işleme aşamalarından geçirmemiz lazım. Bu aşamaları birkaç adımda sıralayabiliriz.\n",
        "1. İlki noktalama işaretleri. Veri kümesi içerisinde noktalama işaretleri kelimelerin bir parçası olmadığı için veri kümesinden silinmelidir. İleride deyineceğim ama biz temelinde kelimeleri 0 ve daha farklı sayılardan oluşan bir matris haline getirmeye çalışıyoruz. Burada eğer noktalama işaretlerini yok etmezsek bilgisayar bunları da bir sayı olarak düşünür ve yanıltıcı bir durum ortaya çıkar. Bu istediğimiz bir durum değil.\n",
        "2. İkincisi büyük harfler. İlk harfi büyük olan  Mehmet ile ilk harfi küçük olan mehmet farklı anlamamı gelmeli? Tabiki hayır. Bu nedenle elimizde ki veri kümesinde büyük harfleri küçük harflere çevirerek bu ayrımı ortadan kaldırmış oluyoruz.\n",
        "3. Tıpkı büyük harf küçük harf durumunda olduğu gibi Ahmet’in ya da Ahmet’e gibi kelimelerin birbirinden farklılığı olmaması gerekir. Bu nedenle bir kelimenin başına veya sonuna gelen bütün eklerden kurtulmuş olması gerekir. Yani kelimelerin sadece köklerden oluşmasını isteriz.\n",
        "4. Birleştirici Kelimeler. Bu durumunda diğerlerinden hiçbir farkı yok. ‘ve’, ‘veya’, ‘ile’ gibi bağlaçların silinmesi gerekir. Burada bütün adımlarda yaptığımız temel amaç aynıdır. Bu adımların sonunda geriye sadece kelimelerin kökleri kalmalıdır. Bunun dışında hiçbir şey istemiyoruz. Şimdi gelin bu işlerin Python kodlamasında nasıl yapıldığına bir bakalım.\n"
      ]
    },
    {
      "metadata": {
        "id": "ZecTmv8Bh_G6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Stop Words\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Stemmer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "# Regular Expressions\n",
        "import re\n",
        "\n",
        "sentenceList = []\n",
        "for i in range(len(sentenceInput)):\n",
        "    # Regular Expressions\n",
        "    comment = re.sub('[^a-zA-Z]', ' ', sentenceInput[i]) #-> 1.Adım\n",
        "    # Upper Case Problem\n",
        "    comment = comment.lower() #-> 2. Adım\n",
        "    # Splitting\n",
        "    comment = comment.split() #-> 3.Adım\n",
        "    # Stemming and Stop Words\n",
        "    comment = [ps.stem(kelime) for kelime in comment if not kelime in set(stopwords.words('english'))] # -> 4. Adım\n",
        "    \n",
        "    comment = ' '.join(comment)\n",
        "    sentenceList.append(comment)\n",
        "    \n",
        "temporaryResult = pd.DataFrame(data=sentenceList)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "estPIRqf_ed4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Burada ilk 3 adım gerekli kütüphanelerin çağrıldığı kısımdır. Bizi asıl ilgilendiren ‘for’ döngüsünün başladığı kısım. Bu döngünün amacı her cümleye tek tek erişmektir. Yani bu döngü her çalıştığında biz bir satırda ki cümleye ulaşıyoruz. Dolayısıyla o cümleye ulaştığımıza göre her şeyi tek bir döngü içerisinde yapabiliriz.\n",
        "\n",
        "1.Adım(Regular Expressions) -> İlk aşamada düzenli ifadeler(Regular Expressions) denilen bir kavram ortaya çıkıyor. Bu kavram bilgisayar bilimlerinde çok ayrıntılı bir şekilde anlatılan ve esasında biz bir programlama dili oluştururken o dile kurallar verdiğimiz kısımda kendini gösteriyor. Burada tek bilmemiz gereken şey 2 büyük parantez içerisinde ki ifadede a-z bütün küçük harflerin, A-Z bütün büyük ifadelerin, tırnak işaretinin bütün küçük harflerin ve bütün büyük harflerin dışında kalan ifadeleri temsil ediyor olmasıdır. İkinci kısımda ki boşluk karakteri, bu ifadelerden herhangi biriyle karşılaşılır ise o ifadeyi boşluk karakteri ile yer değiştirecektir. Bunu yaptığımız zaman bütün noktalama işaretlerinden kurtulmuş oluyoruz.\n",
        "\n",
        "2.Adım(Upper Case Problem) -> Hatırlarsanız ikinci adımımız büyük harflerden kurtulmaktı. Burada da ikinci adımda lower() fonksiyonunu kullanarak bütün büyük harfleri küçük harfler ile yer değiştirmiş olduk.\n",
        "\n",
        "3.Adım(Splitting) -> Daha sonrasında yapmamız gereken şey kelime eklerinden ve bağlaç gibi cümleden bağımsız olan kelimelerden kurtulmak. Bunu yapmadan önce elimizde her bir kelimenin olduğu bir liste olması gerekiyor. Split() fonksiyonu cümledeki her bir kelimeyi bir listeye aktarıyor. Şimdi artık bu kelimelerin her birine tek tek ulaşarak o kelimenin üzerinde işlem yapabiliriz.\n",
        "\n",
        "4.Adım(Stemming and Stop Words) -> Bir alt satırda ki kod parçasında bir döngü ile her bir kelimeye tek tek ulaşıyor ve eğer bu kelime bir stopwords değilse o zaman stem() fonksiyonu ile başında veya sonunda bulunan eklerden kurtulmasını sağlıyoruz. Eğer bir stopwords ise o zaman kelimeyi tamamen yok ediyor.\n",
        "Bütün bu adımların sonunda artık elimizde o 4 adımda istediğimiz sadece köklerden oluşan bir veri kümesi olması gerekiyor. Bakalım gerçekten öyle olmuş mu?"
      ]
    },
    {
      "metadata": {
        "id": "dOebcgsp_pCM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1092
        },
        "outputId": "8a4d1537-fa26-41de-940c-de464f577aad"
      },
      "cell_type": "code",
      "source": [
        "print(temporaryResult)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                      0\n",
            "0                 constitut lawyer warn trump administr\n",
            "1        trump dossier russia link part special counsel\n",
            "2     reveal trump financi backer paid playmat hush ...\n",
            "3     cnn look trump moment ago end wolf blitzer car...\n",
            "4        donald trump hail f fighter jet liter cant see\n",
            "5     donald trump unveil tax plan could add billion...\n",
            "6                mike penc presid trump run mate vision\n",
            "7                                       trump peopl get\n",
            "8     julian assang trump wont allow win clinton isi...\n",
            "9             turnbul abe push tpp despit trump sceptic\n",
            "10                                choic trump oligarchi\n",
            "11                     donald trump day progress report\n",
            "12          donald trump enemi rule law chines judg say\n",
            "13    implos trump campaign insid report think bunke...\n",
            "14                        trump visit kid famili impact\n",
            "15    trump campaign admit tri steal elect voter sup...\n",
            "16    trump make special announc homeless ladi attac...\n",
            "17        lebron jame take aim donald trump nfl comment\n",
            "18    former classmat trump smack son hard knock flo...\n",
            "19                                   trump withdraw tpp\n",
            "20                   donald trump begin asia trip japan\n",
            "21                                   gambl unknow trump\n",
            "22    donald trump sign execut order obama climat po...\n",
            "23                               troubl trump asia trip\n",
            "24        trump polici may boost number north coast uni\n",
            "25                     donald trump one year sinc elect\n",
            "26              uber ceo quit trump busi advisori group\n",
            "27               opinion donald trump conflict interest\n",
            "28       women tweet sexual assault stori wake trumptap\n",
            "29                  trump advisor counsel free commerci\n",
            "...                                                 ...\n",
            "2747  harder clinton support respect trump backer vi...\n",
            "2748  zambia fear health program suffer trump foreig...\n",
            "2749       shout presid california follow trump victori\n",
            "2750         disgust celebr unleash vulgar attack trump\n",
            "2751          donald trump say blame side charlottesvil\n",
            "2752     donald trump arriv washington dc ahead inaugur\n",
            "2753      trump advanc controversi north dakota pipelin\n",
            "2754                 busi gaug fallout trump travel ban\n",
            "2755      wall street flat lack trump tax polici detail\n",
            "2756                 market unmov trump tax plan unveil\n",
            "2757             trump brexit direct histori proce apac\n",
            "2758                                              trump\n",
            "2759  farmer export keenli await donald trump trade ...\n",
            "2760      donald trump russia link dwarf waterg scandal\n",
            "2761          trump put lobbi restrict administr offici\n",
            "2762   anoth big week washington us presid donald trump\n",
            "2763              donald trump continu attack fake news\n",
            "2764            donald trump suggest major incid sweden\n",
            "2765  trump caus uncertainti climat chang confer fry...\n",
            "2766                morn market wall street trump trade\n",
            "2767  trump polici provid australia busi event opportun\n",
            "2768  donald trump granddaught arabella recit chines...\n",
            "2769                 manchest attack trump condemn leak\n",
            "2770    trump presid creat mountain salt butthurt liber\n",
            "2771  milo dartmouth colleg donald trump real men po...\n",
            "2772         pimco warn trump protectionist polici risk\n",
            "2773                trump total accept elect result win\n",
            "2774  betsi devo confirm us educ secretari donald trump\n",
            "2775      us hous approv new russia sanction defi trump\n",
            "2776           trump say bring price wall mexico border\n",
            "\n",
            "[2777 rows x 1 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "THi3jm8-PXaQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Evet, gerçekten veri kümesi içerisinde sadece köklerin olduğunu ve herhangi bir bağlaç kalmadığını görebiliyoruz. Tam olarak istediğimiz durum buydu. Artık veri kümesini bilgisayarın anlayabileceği 0 ve 1 lerden oluşan bir matris haline getirebiliriz.\n",
        "\n",
        "Bunun için kullandığımız Python kütüphanesi Scikit-Learn. Bu kütüphanenin içerisindeki CountVectorizer sınıfını kullanarak bu işlemi yapabiliyoruz. Bu sınıf veri kümesinde ki bütün kelimeleri alıp birer değişken haline getiriyor ve her bir cümlede o kelimenin olup olmadığına göre 0 ve 1 sayısını veriyor. En nihayetinde elimizde çok fazla sayıda 0 ve çok az sayıda 1’lerden oluşan bir sparse matris oluşuyor.\n"
      ]
    },
    {
      "metadata": {
        "id": "0NLkNMT1u-sX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Vectorized All Data\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(sentenceList)\n",
        "\n",
        "result = pd.DataFrame(data=X.toarray())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zvDlR5bLQA6I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Şu noktadan itibaren bu veri kümesi artık bilgisayarın anlayabileceği bir model haline geldi."
      ]
    },
    {
      "metadata": {
        "id": "JxVOM_dEPzH5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Test veri kümemiz ile eğitim veri kümemiz birbirinden tamamen ayrı. Bu nedenle bu işlemlerin birebir aynısı test veri kümesi içinde uygulanmıştır.\n"
      ]
    },
    {
      "metadata": {
        "id": "XUGxSi7DdrQg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Test Data\n",
        "testSentenceList = []\n",
        "for i in range(len(testList)):\n",
        "    # Regular Expressions\n",
        "    comment = re.sub('[^a-zA-Z]', ' ', testList[i])\n",
        "    # Upper Case Problem\n",
        "    comment = comment.lower()\n",
        "    # Splitting\n",
        "    comment = comment.split()\n",
        "    # Stemming and Stop Words\n",
        "    comment = [ps.stem(kelime) for kelime in comment if not kelime in set(stopwords.words('english'))]\n",
        "    \n",
        "    comment = ' '.join(comment)\n",
        "    testSentenceList.append(comment)\n",
        "\n",
        "X = vectorizer.transform(testSentenceList)\n",
        "\n",
        "testResult = pd.DataFrame(data=X.toarray())\n",
        "# PCA for Test\n",
        "#testResult = pca.transform(testResult)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xrVkk9uuQnlj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Derin Öğrenme\n",
        "Nihayet artık veri kümesi bir model oluşturmaya hazır hale gelmiştir. Modelimiz bir derin öğrenme modelidir. Dolayısıyla derin öğrenmenin ne olduğundan kısaca bahsetmekte fayda var. Derin öğrenme, basit yapay sinir ağlarının daha karmaşık hale getirilmişi ve her bir katmana istediğimiz gibi müdahale edebildiğimiz bir yapısıdır. Karmaşıklık arttıkça derin öğrenmeye olan ihtiyaçta artıyor.\n"
      ]
    },
    {
      "metadata": {
        "id": "SmbmZpvrSKyX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "[![image](https://i.hizliresim.com/9a0AGO.jpg)](https://hizliresim.com/9a0AGO)"
      ]
    },
    {
      "metadata": {
        "id": "rGjMpTxjTayP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Şimdi nasıl bir model tasarladığıma kısaca bir bakalım. Elimizde 1 tane giriş katmanı, 1 tane saklı katman ve 2 tane seyreltme katmanı bulunmaktadır. Giriş ve saklı katman 3000’er tane nörona sahiptir. 2 tane seyreltme katmanımız var ve bu katmanlar 0.5 eşik değerine sahip. Bu seyreltme katmanlarının temel amacı ezberlemeyi azaltmaktır. Bu istemediğimiz bir durum. Dolayısıyla bu seyreltme katmanları eşik değerini 0.5 olarak girdiğimiz için burada 1500 tane olan nöron sayısını rasgele olarak yarıya düşürüyor. Bu noktadan sonra sonucu gerçek ya da yalan olarak belirlenecek olan tek bir nörona sahip bir tane çıkış katmanı oluşturuluyor. Daha geniş bir açıdan baktığımız zaman 1 tane giriş katmanımız, 1 tane saklı katmanımız ve 1 tanede çıkış katmanımız oluşuyor. Şimdi bunun nasıl kodlandığına bakalım.\n"
      ]
    },
    {
      "metadata": {
        "id": "Bt5dK1M4Ugdx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Öncelikle kullanılan kütüphane Google’ın bir çalışanı tarafından tasarlanan Keras kütüphanesidir. Bu kütüphane arka planında Tensorflow denen başka bir kütüphaneyi kullanmaktadır. Bu kütüphaneyi ise Google şirketinin direk kendisi tasarladı. Bu iki kütüphane sayesinde Python üzerinden Derin öğrenme modelleri oluşturabiliyoruz. Bunlardan başka kütüphanelerde vardır fakat en çok kullanılan iki kütüphane bu ikisidir.\n",
        "\n",
        "Burada odaklanmamız gereken kısımlar Dense ve Dropout fonksiyonlarının olduğu kısımlardır. 1 numaralı kısımda ilk parametre olarak 1500 sayısını girerek nöron sayısını girmiş olduk. \n",
        "\n",
        "İkinci parametre kernel_initializer başlangıçta hangi istatistiksel dağılımı kullanarak nöronlara katsayılar vereceğimizi belirlediğimiz kısım. Burada uniform dağılımı kullandım.\n",
        "\n",
        "Üçüncüsü ise aktivasyon fonksiyonudur, bu fonksiyon girişteki katsayıyı alıp kendi fonksiyonunda işlem uygulayarak çıktı katsayısı elde etmemizi sağlıyor. Daha sonra bu çıktı katsayısı ya direk sonucun kendisi olur ya da bir başka katmanın girdisi olur. Burada aktivasyon fonksiyonu olarak en popüleri olan Sigmoid fonksiyonu kullanmayı tercih ettim. Bunun temel sebebi doğrusal olmayan problemlerde iyi bir aktivasyon fonksiyonu olması. \n",
        "\n",
        "En son parametre olarakta elimizdeki veri kümesinin değişken sayısını vermemiz gerekiyor. Hatırlarsanız bu sayı bileşen analizi yaparken belirlediğimiz 2000 sayısıydı.\n",
        "\n",
        "Daha sonra Dropout katmanını oluşturuyoruz ve eşik değeri olarak 0.5 giriyoruz. 2 numaralı kısımda 1 numaralı kısımda yaptığımız şeyden farklı olan hiçbir şey yapmıyoruz. 3 numaralı kısımda ise sonucu doğru ya da yalan olan 1 tane nörona sahip bir katman oluşturuyoruz.\n",
        "\n",
        "Bundan sonraki aşamada bir optimizasyon fonksiyonu belirlememiz gerekiyor. Bu fonksiyonun amacı ise her bir geri beslemede nöronların katsayılarını daha optimal hale getirip doğruluk yüzdemizi arttırmayı amaçlamak. Bakalım kodlarımız nasıl çalışmış.\n"
      ]
    },
    {
      "metadata": {
        "id": "YKks-ZQ9eCuv",
        "colab_type": "code",
        "outputId": "33d1549f-c600-4b2b-956c-7fff36e6e8ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1633
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras import optimizers\n",
        "import datetime\n",
        "\n",
        "time = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
        "\n",
        "kerasboard = TensorBoard(log_dir=\"/tmp/tensorboard/{}\".format(time))\n",
        "\n",
        "#result = result.reshape((2777, 1, 2000))\n",
        "#testResult = testResult.reshape((489, 1, 2000))\n",
        "\n",
        "classifier = Sequential()\n",
        "\n",
        "classifier.add(Dense(1500, kernel_initializer=\"he_normal\", activation='sigmoid', input_dim=3854)) #1 -> Giriş Katmanı\n",
        "#classifier.add(LSTM(1500, input_shape=(1, 2000), activation='sigmoid',return_sequences=False))\n",
        "classifier.add(Dropout(0.50))\n",
        "\n",
        "\n",
        "classifier.add(Dense(1500,kernel_initializer='he_normal' ,activation='sigmoid')) #2 -> Saklı Katman\n",
        "classifier.add(Dropout(0.50))\n",
        "\n",
        "\n",
        "classifier.add(Dense(1, kernel_initializer=\"he_normal\", activation='sigmoid')) #3 -> Çıkış Katmanı\n",
        "\n",
        "adam = optimizers.Adam(lr=0.0001, decay=0.00008)\n",
        "\n",
        "classifier.compile(optimizer=adam,loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "classifier.fit(result, targetInputFloat, epochs=40, callbacks=[kerasboard])\n",
        "\n",
        "prediction = classifier.predict(testResult)\n",
        "\n",
        "prediction = (prediction > 0.5)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "acScoreNN = accuracy_score(lb.fit_transform(testDataForComparison), prediction)\n",
        "print(\"Accuracy : \" + str(acScoreNN))\n",
        "\n",
        "probaNN = classifier.predict_proba(testResult)\n",
        "print(\"NN AUC \" + str(metrics.roc_auc_score(lb.fit_transform(testDataForComparison), probaNN)))\n",
        "\n",
        "print(\"tensorboard --logdir=\"+kerasboard.log_dir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.7594 - acc: 0.5492\n",
            "Epoch 2/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.7297 - acc: 0.5668\n",
            "Epoch 3/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.7140 - acc: 0.5686\n",
            "Epoch 4/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.7122 - acc: 0.5697\n",
            "Epoch 5/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.6943 - acc: 0.5870\n",
            "Epoch 6/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.6742 - acc: 0.5906\n",
            "Epoch 7/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.6694 - acc: 0.5862\n",
            "Epoch 8/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.6492 - acc: 0.6136\n",
            "Epoch 9/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.6351 - acc: 0.6331\n",
            "Epoch 10/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.6191 - acc: 0.6554\n",
            "Epoch 11/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.6025 - acc: 0.6799\n",
            "Epoch 12/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.5784 - acc: 0.7018\n",
            "Epoch 13/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.5654 - acc: 0.7105\n",
            "Epoch 14/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.5451 - acc: 0.7198\n",
            "Epoch 15/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.5175 - acc: 0.7436\n",
            "Epoch 16/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.4928 - acc: 0.7631\n",
            "Epoch 17/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.4795 - acc: 0.7717\n",
            "Epoch 18/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.4629 - acc: 0.7764\n",
            "Epoch 19/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.4441 - acc: 0.7955\n",
            "Epoch 20/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.4272 - acc: 0.8016\n",
            "Epoch 21/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.4219 - acc: 0.8066\n",
            "Epoch 22/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.4055 - acc: 0.8225\n",
            "Epoch 23/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.3875 - acc: 0.8228\n",
            "Epoch 24/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.3689 - acc: 0.8365\n",
            "Epoch 25/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.3687 - acc: 0.8380\n",
            "Epoch 26/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.3656 - acc: 0.8387\n",
            "Epoch 27/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.3561 - acc: 0.8340\n",
            "Epoch 28/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.3677 - acc: 0.8326\n",
            "Epoch 29/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.3391 - acc: 0.8506\n",
            "Epoch 30/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.3375 - acc: 0.8470\n",
            "Epoch 31/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.3145 - acc: 0.8671\n",
            "Epoch 32/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.3282 - acc: 0.8617\n",
            "Epoch 33/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.3061 - acc: 0.8624\n",
            "Epoch 34/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.3044 - acc: 0.8675\n",
            "Epoch 35/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.2969 - acc: 0.8714\n",
            "Epoch 36/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.3018 - acc: 0.8653\n",
            "Epoch 37/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.2965 - acc: 0.8704\n",
            "Epoch 38/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.2996 - acc: 0.8714\n",
            "Epoch 39/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.2814 - acc: 0.8801\n",
            "Epoch 40/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.2688 - acc: 0.8869\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy : 0.8261758691206544\n",
            "NN AUC 0.9125982876113927\n",
            "tensorboard --logdir=/tmp/tensorboard/2019_04_21_06_30_35\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PEt5LISjPRy8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard.notebook\n",
        "\n",
        "#Activate Tensorboard\n",
        "%tensorboard --logdir=/tmp/tensorboard/2019_03_30_10_34_18"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r1wi9YUKjftV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Saving the best model with Pickle (Neural %84.04)\n",
        "import pickle\n",
        "pickle.dump(classifier, open(\"NeuralNews\", 'wb'))\n",
        "\n",
        "loading = pickle.load(open(\"NeuralNews\", 'rb'))\n",
        "predictionPickleNeural = loading.predict(testResult)\n",
        "\n",
        "predictionPickleNeural = (predictionPickleNeural > 0.5)\n",
        "\n",
        "acScorePickleNeural = accuracy_score(lb.fit_transform(testDataForComparison), predictionPickleNeural)\n",
        "print(\"Accuracy Pickle Neural : \" + str(acScorePickleNeural))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}