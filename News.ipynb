{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "News.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mertyyanik/Natural-Language-Processing-Real-or-Fake/blob/master/News.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "N0MgjiC5sUzr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Doğal Dil İşleme - Gerçek ya da Sahte?\n",
        "Doğal Dil işleme (Natural Language Processing), basit hali ile yapay zeka ve dil biliminin birleşmesi ile ortaya çıkmış bir kavramdır. Bu kavram herkesin günlük konuşma dilinin bilgisayara öğretilmesi ve bazı çalışmaların bilgisayara yaptırılmasına sebep olmuştur. Günümüze yakın tarihlere baktığımız zaman bilgisayarların güçlenmesini takip eden makine öğrenmesi modelleri daha da gelişmiş ve bir çok alanda kullanılmaya başlanmıştır. Bu alanlardan biriside doğal dil işlemedir. Doğal dil işlemeyi makine öğrenmesi modelleri ile birleştirdiğimiz zaman ortaya bir cümlenin kime ait olduğunu ya da bir cümleyi söyleyen kişinin nasıl bir ruh haline sahip olduğunu anlayabilir noktaya gelmiş bulunmaktayız.\n",
        "\n",
        "Bizimde amacımız buna benzer bir çalışma yapmaktır. Elimizde haber sitelerinin yazmış olduğu haber başlıkları ve bu başlıkların gerçek ya da sahte olduğunu gösteren bir veri kümesi bulunmaktadır. Derin öğrenme yöntemlerini kullanarak bu veri kümesini bilgisayara öğretecek ve gelecekte yazılmış bir haber başlığının gerçek mi yoksa sahte mi olduğunu anlamaya çalışacağız."
      ]
    },
    {
      "metadata": {
        "id": "FIkAaBFktE21",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Bunun için Google'ın geliştirmiş olduğu Colab ortamını kullanıyoruz. Bu ortamın avantajı derin öğrenme yöntemlerinde uygulamaları eğitirken **TPU**'ları kullanıyor olmasıdır. TPU'lar, GPU veya CPU'ya göre 15 kat daha hızlıdır. Bu da bizi büyük bir oranda zaman maliyetinden kurtarmaktadır."
      ]
    },
    {
      "metadata": {
        "id": "Wb9SkSN9S5UF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Öncelikle elimizde ki veri kümelerini Colab ortamına aktarmamız gerekiyor. Bunu yapmadan önce ortamın Google Drive ile olan bağlantısını onaylamamız lazım. Aşağıda ki kod bloğunu çalıştırdığımız zaman ekranda bir link belirmektedir bu linke tıklayarak karşımıza çıkan kodu output kısmına yapıştırarak Google Drive ile olan bağlantıyı sağlamış oluyoruz."
      ]
    },
    {
      "metadata": {
        "id": "29Lt9X4PbrWI",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        " \n",
        "from pydrive.auth import GoogleAuth\n",
        " \n",
        "from pydrive.drive import GoogleDrive\n",
        " \n",
        "from google.colab import auth\n",
        " \n",
        "from oauth2client.client import GoogleCredentials\n",
        " \n",
        "auth.authenticate_user()\n",
        " \n",
        "gauth = GoogleAuth()\n",
        " \n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "itSyx3Sql4cA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Bundan sonraki aşama veri kümelerini Google Drive'dan indirmektir. Burada ki önemli nokta dosya numarasının (File ID) elde edilmesidir. Bu numaraya  Google Drive üzerinden veri kümesinin paylaş butonuna basarak ulaşabilirsiniz."
      ]
    },
    {
      "metadata": {
        "id": "Zb5eh3gMcXGh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Test.csv Dosyası\n",
        "from google.colab import files\n",
        "\n",
        "file_id = '1Zx7P0t9lQLP4aj4YOUaZDxNRrSkZOTk4'\n",
        "\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "\n",
        "downloaded.GetContentFile('test.csv')\n",
        "\n",
        "#Test.txt Dosyası\n",
        "from google.colab import files\n",
        " \n",
        "file_id = '1S2-N2HSIgtZpSzPmAEZWmVxV2YlOzCzs'\n",
        " \n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        " \n",
        "downloaded.GetContentFile('test.txt')\n",
        "\n",
        "#clean_real-Train.txt Dosyası\n",
        "from google.colab import files\n",
        " \n",
        "file_id = '1pnsf7izWzNYlA41xcWrCXQ5SSFe5JCUF'\n",
        " \n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        " \n",
        "downloaded.GetContentFile('clean_real-Train.txt')\n",
        "\n",
        "#clean_fake-Train.txt Dosyası\n",
        "from google.colab import files\n",
        " \n",
        "file_id = '1Mc1m6S6WCJuRkMBWM9_LZGHSzJIw2-e0'\n",
        " \n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        " \n",
        "downloaded.GetContentFile('clean_fake-Train.txt')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_Fe5yhr9m_eg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Bu noktadan sonra artık kendi Python kodlamanızı istediğiniz gibi yazabiliriz.\n",
        "\n",
        "Öncelikle gerekli kütüphaneleri yükleyelim."
      ]
    },
    {
      "metadata": {
        "id": "uyfureqInTdY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R0ZYcTFZMivK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Text dosyasında bulunan veri dosyalarımızı ayrı ayrı listelere aktarıyoruz. Böylece Python'ın nimetlerinden faydalanabileceğiz."
      ]
    },
    {
      "metadata": {
        "id": "y_8AGswcdRCv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Reading Data\n",
        "input1Data = open('clean_real-Train.txt', 'r+')\n",
        "input2Data = open('clean_fake-Train.txt', 'r+')\n",
        "testData = open('test.txt')\n",
        "\n",
        "testDataForComparison = pd.read_csv('test.csv').iloc[:, 1:]\n",
        "\n",
        "input1List = []\n",
        "input2List = []\n",
        "testList = []\n",
        "\n",
        "read = input1Data.readline()\n",
        "read2 = input2Data.readline()\n",
        "read3 = testData.readline()\n",
        "rowCount = 0\n",
        "while read != \"\":\n",
        "    input1List.append(read)\n",
        "    read = input1Data.readline()\n",
        "    \n",
        "while read2 != \"\":\n",
        "    input2List.append(read2)\n",
        "    read2 = input2Data.readline()\n",
        "\n",
        "while read3 != \"\":\n",
        "    testList.append(read3)\n",
        "    read3 = testData.readline()\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AM0hwXewMn0W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Daha sonra iki farklı veri kümesinde bulunan 'real' ve 'fake' veri kümelerini birleştiriyoruz. Real olanlara 1, fake olanlara 0 yazıyoruz."
      ]
    },
    {
      "metadata": {
        "id": "HtMraR2NhkjO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Real and Fake data combined.    \n",
        "liste = ['real' for i in range(len(input1List))]\n",
        "realInput = np.column_stack((input1List, liste))\n",
        "\n",
        "\n",
        "liste = ['fake' for i in range(len(input2List))]\n",
        "fakeInput = np.column_stack((input2List, liste))\n",
        "\n",
        "mainInput = realInput.tolist() + fakeInput.tolist()\n",
        "random.shuffle(mainInput)\n",
        "\n",
        "sentenceInput = []\n",
        "targetInput = []\n",
        "targetInputFloat = []\n",
        "for i in range(len(mainInput)):\n",
        "    sentenceInput.append(mainInput[i][0])\n",
        "    \n",
        "for i in range(len(mainInput)):\n",
        "    targetInput.append(mainInput[i][1])\n",
        "    \n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "lb = LabelEncoder()\n",
        "targetInputFloat = lb.fit_transform(targetInput)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3D3wvtrBNfFD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Veri Ön İşleme\n",
        "Bundan sonraki aşama veri ön işlemenin başladığı kısım. Veri kümesini bilgisayarın anlayabileceği sayılara çevirmeden önce bakıldığı zaman görülen bazı yanlışlar yok mu? Tabiki var. Bu yanlışların ne olduğundan bahsetmemiz ve en nihayetinde bunlardan kurtulmamız lazım. Yani veri kümemizi bazı veri ön işleme aşamalarından geçirmemiz lazım. Bu veri ön işleme aşamalarını birkaç adımda sıralayabiliriz.\n",
        "1. Bunlardan ilki noktalama işaretleri. Veri kümesi içerisinde noktalama işaretleri kelimelerin bir parçası olmadığı için veri kümesinden silinmelidir. İleride deyineceğim ama biz temelinde kelimeleri 0 ve daha farklı sayılardan oluşan bir matris haline getirmeye çalışıyoruz. Burada eğer noktalama işaretlerini yok etmezsek bilgisayar bunları da bir sayı olarak düşünür ve yanıltıcı bir durum ortaya çıkar. Bu istediğimiz bir durum değil.\n",
        "2. İkincisi büyük harfler. İlk harfi büyük olan  Mehmet ile ilk harfi küçük olan mehmet farklı anlamamı gelmeli? Tabiki hayır bu nedenle elimizde ki veri kümesinde büyük harfleri küçük harflere çevirerek bu ayrımı ortadan kaldırmış oluyoruz.\n",
        "3. Tıpkı büyük harf küçük harf durumunda olduğu gibi Ahmet’in ya da Ahmet’e gibi kelimelerin birbirinden farklılığı olmaması gerekir. Bu nedenle bir kelimenin başına veya sonuna gelen bütün eklerden kurtulmuş olması gerekir. Yani kelimelerin sadece köklerden oluşmasını isteriz.\n",
        "4. Birleştirici Kelimeler. Bu durumunda diğerlerinden hiçbir farkı yok. ‘ve’, ‘veya’, ‘ile’ gibi bağlaçların silinmesi gerekir. Burada bütün adımlarda yaptığımız temel amaç aynıdır. Bu adımların sonunda geriye sadece kelimelerin kökleri kalmalıdır. Bunun dışında hiçbir şey istemiyoruz. Şimdi gelin bu işlerin Python kodlamasında nasıl yapıldığına bir bakalım.\n",
        "\n",
        "Burada ilk 3 adım gerekli kütüphanelerin çağrıldığı kısımdır. Bizi asıl ilgilendiren ‘for’ döngüsünün başladığı kısım. Bu döngünün amacı her cümleye tek tek erişmektir. Yani bu döngü her çalıştığında biz bir satırda ki cümleye ulaşıyoruz. Dolayısıyla o cümleye ulaştığımıza göre her şeyi tek bir döngü içerisinde yapabiliriz. İlk aşamada düzenli ifadeler denilen bir kavram ortaya çıkıyor. Bu kavram bilgisayar bilimlerinde çok ayrıntılı bir şekilde anlatılan ve esasında biz bir programlama dili oluştururken o dile kurallar verdiğimiz kısımda kendini gösteriyor. Burada tek bilmemiz gereken şey 2 büyük parantez içerisinde ki ifadede a-z bütün küçük harflerin, A-Z bütün büyük ifadelerin ve tırnak işaretinin bütün küçük harflerin ve bütün büyük harflerin dışında kalan ifadeleri temsil ediyor olmasıdır. İkinci kısımda ki boşluk karakteri, bu ifadelerden herhangi biriyle karşılaşılır ise o ifadeyi boşluk karakteri ile yer değiştirecektir. Bunu yaptığımız zaman bütün noktalama işaretlerinden kurtulmuş oluyoruz.\n",
        "\n",
        "Hatırlarsanız ikinci adımımız büyük harflerden kurtulmaktı. Burada da ikinci adımda lower() fonksiyonunu kullanarak bütün büyük harfleri küçük harfler ile yer değiştirmiş olduk.\n",
        "Daha sonrasında yapmamız gereken şey kelime eklerinden ve bağlaç gibi cümleden bağımsız olan kelimelerden kurtulmak. Bunu yapmadan önce elimizde her bir kelimenin olduğu bir liste olması gerekiyor. Split() fonksiyonu cümledeki her bir kelimeyi bir listeye aktarıyor. Şimdi artık bu kelimelerin her birine tek tek ulaşarak o kelimenin üzerinde işlem yapabiliriz.\n",
        "Bir alt satırda ki kod parçasında bir döngü ile her bir kelimeye tek tek ulaşıyor ve eğer bu kelime bir stopwords değilse o zaman stem() fonksiyonu ile başında veya sonunda bulunan eklerden kurtulmasını sağlıyoruz. Eğer bir stopwords ise o zaman kelimeyi tamamen yok ediyor.\n",
        "Bütün bu adımların sonunda artık elimizde o 4 adımda istediğimiz sadece köklerden oluşan bir veri kümesi olması gerekiyor. Bakalım gerçekten öyle olmuş mu?\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "ZecTmv8Bh_G6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1126
        },
        "outputId": "b18c43ed-aeef-4fb5-9b46-a1c069cc114c"
      },
      "cell_type": "code",
      "source": [
        "# Stop Words\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Stemmer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "# Regular Expressions\n",
        "import re\n",
        "\n",
        "sentenceList = []\n",
        "for i in range(len(sentenceInput)):\n",
        "    # Regular Expressions\n",
        "    comment = re.sub('[^a-zA-Z]', ' ', sentenceInput[i]) #-> 1.Adım\n",
        "    # Upper Case Problem\n",
        "    comment = comment.lower() #-> 2. Adım\n",
        "    # Splitting\n",
        "    comment = comment.split() #-> 3.Adım\n",
        "    # Stemming and Stop Words\n",
        "    comment = [ps.stem(kelime) for kelime in comment if not kelime in set(stopwords.words('english'))] # -> 4. Adım\n",
        "    \n",
        "    comment = ' '.join(comment)\n",
        "    sentenceList.append(comment)\n",
        "    \n",
        "temporaryResult = pd.DataFrame(data=sentenceList)\n",
        "print(temporaryResult)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "                                                      0\n",
            "0                    donald trump begin asia trip japan\n",
            "1          donald trump call media enemi american peopl\n",
            "2                   obama tell trump presid famili busi\n",
            "3     bad jewish loser lament trump victori bbc news...\n",
            "4     face nobodi care wikileak trump doesnt tempera...\n",
            "5                   fool trump new messiah includ video\n",
            "6                                   deep trump truth go\n",
            "7                 donald trump support fight rig system\n",
            "8     donald trump hit shadi voter intimid lawsuit a...\n",
            "9     donald trump plan make america great could cos...\n",
            "10    dilbert creator endors donald trump simpl reas...\n",
            "11              trump prais poll said rig point florida\n",
            "12         trump slam stein recount push scam say elect\n",
            "13           mcmaster deni trump reveal classifi inform\n",
            "14                      trump pick loyalist econom post\n",
            "15    cnn catch trump make sure wife vote twitter er...\n",
            "16            hillari plan violent mob attack trump win\n",
            "17             trump promis billion black latino commun\n",
            "18    north korea trump condemn rogu nation hostil a...\n",
            "19               donald trump travel ban due argu court\n",
            "20    hillari clinton brilliantli forc broke trump s...\n",
            "21          batavia replica trumpet play shipwreck site\n",
            "22                  donald trump defend firm bold decis\n",
            "23            donald trump endors roy moor senat candid\n",
            "24    farmer export keenli await donald trump trade ...\n",
            "25       donald trump anthoni scaramucci vow crack leak\n",
            "26          christian evangel believ trump save america\n",
            "27    intercept out neocon democrat smear trump puti...\n",
            "28    putin hillari clinton start world war trump wa...\n",
            "29                  trump trade polici easier said done\n",
            "...                                                 ...\n",
            "2747                stephen king got real trump support\n",
            "2748          mississippi church burn vandal vote trump\n",
            "2749  french presid emmanuel macron shake hand doanl...\n",
            "2750  donald trump republican unveil tax cut bill ha...\n",
            "2751  donald trump may broken law avoid pay feder in...\n",
            "2752     donald trump confirm cancel meet mexico presid\n",
            "2753           donald trump lash senat richard blumenth\n",
            "2754  bazinga donald trump ten point swing poll fbi ...\n",
            "2755                            donald trump rush stage\n",
            "2756  break trump announc devast cut social secur re...\n",
            "2757                          trump investig comey fire\n",
            "2758               trump per cent will speak oath comey\n",
            "2759  donald trump trade threat north korea nuclear ...\n",
            "2760    trump presid creat mountain salt butthurt liber\n",
            "2761   trump deliv afghanistan strategi us militari ask\n",
            "2762   trump renew tough rhetor north korea talk missil\n",
            "2763  georg soro trump win popular vote landslid hil...\n",
            "2764  donald trump apologis vulgar comment criticis ...\n",
            "2765          trump sign buy american hire execut order\n",
            "2766              donald trump visit cover chines media\n",
            "2767  donald trump pressur side see messi pari withdraw\n",
            "2768  chicago hit back strip trump honorari street d...\n",
            "2769            trump prais condemn syria missil strike\n",
            "2770                                              trump\n",
            "2771                        mani never vote trump elect\n",
            "2772  republican win congress contest seen test dona...\n",
            "2773  donald trump call expand us nuclear weapon capabl\n",
            "2774         donald trump threaten nbc broadcast licenc\n",
            "2775               north korea say trump suicid mission\n",
            "2776          busiest congress ever donald trump outlin\n",
            "\n",
            "[2777 rows x 1 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "THi3jm8-PXaQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Evet, gerçekten veri kümesi içerisinde sadece köklerin olduğunu ve herhangi bir bağlaç kalmadığını görebiliyoruz. Tam olarak istediğimiz durum buydu. Artık veri kümesini bilgisayarın anlayabileceği 0 ve 1 lerden oluşan bir matris haline getirebiliriz.\n",
        "\n",
        "Bunun için kullandığımız Python kütüphanesi Scikit-Learn. Bu kütüphanenin içerisindeki CountVectorizer sınıfını kullanarak bu işlemi yapabiliyoruz. Bu sınıf veri kümesinde ki bütün kelimeleri alıp birer değişken haline getiriyor ve her bir cümlede o kelimenin olup olmadığına göre 0 ve 1 sayısını veriyor. En nihayetinde elimizde çok fazla sayıda 0 ve çok az sayıda 1’lerden oluşan bir sparse matris oluşuyor.\n"
      ]
    },
    {
      "metadata": {
        "id": "0NLkNMT1u-sX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Vectorized All Data\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(sentenceList)\n",
        "\n",
        "result = pd.DataFrame(data=X.toarray())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zvDlR5bLQA6I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Şu noktadan itibaren bu veri kümesi artık bilgisayarın anlayabileceği bir model haline geldi."
      ]
    },
    {
      "metadata": {
        "id": "JxVOM_dEPzH5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Test veri kümemiz ile eğitim veri kümemiz birbirinden tamamen ayrı. Bu nedenle bu işlemlerin birebir aynısı test veri kümesi içinde uygulanmıştır.\n"
      ]
    },
    {
      "metadata": {
        "id": "XUGxSi7DdrQg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Test Data\n",
        "testSentenceList = []\n",
        "for i in range(len(testList)):\n",
        "    # Regular Expressions\n",
        "    comment = re.sub('[^a-zA-Z]', ' ', testList[i])\n",
        "    # Upper Case Problem\n",
        "    comment = comment.lower()\n",
        "    # Splitting\n",
        "    comment = comment.split()\n",
        "    # Stemming and Stop Words\n",
        "    comment = [ps.stem(kelime) for kelime in comment if not kelime in set(stopwords.words('english'))]\n",
        "    \n",
        "    comment = ' '.join(comment)\n",
        "    testSentenceList.append(comment)\n",
        "\n",
        "X = vectorizer.transform(testSentenceList)\n",
        "\n",
        "testResult = pd.DataFrame(data=X.toarray())\n",
        "# PCA for Test\n",
        "#testResult = pca.transform(testResult)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xrVkk9uuQnlj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Derin Öğrenme\n",
        "Nihayet artık veri kümesi bir model oluşturmaya hazır hale gelmiştir. Modelimiz bir derin öğrenme modelidir. Dolayısıyla derin öğrenmenin ne olduğundan kısaca bahsetmekte fayda var. Derin öğrenme, basit yapay sinir ağlarının daha karmaşık hale getirilmişi ve her bir katmana istediğimiz gibi müdahale edebildiğimiz bir yapısıdır. Karmaşıklık arttıkça derin öğrenmeye olan ihtiyaçta artıyor.\n"
      ]
    },
    {
      "metadata": {
        "id": "SmbmZpvrSKyX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "[![image](https://i.hizliresim.com/9a0AGO.jpg)](https://hizliresim.com/9a0AGO)"
      ]
    },
    {
      "metadata": {
        "id": "rGjMpTxjTayP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Şimdi nasıl bir model tasarladığıma kısaca bir bakalım. Elimizde 1 tane giriş katmanı, 1 tane saklı katman ve 2 tane seyreltme katmanı bulunmaktadır. Giriş ve saklı katman 3000’er tane nörona sahiptir. 2 tane seyreltme katmanımız var ve bu katmanlar 0.5 eşik değerine sahip. Bu seyreltme katmanlarının temel amacı ezberlemeyi azaltmaktır. Bu istemediğimiz bir durum. Dolayısıyla bu seyreltme katmanları eşik değerini 0.5 olarak girdiğimiz için burada 1500 tane olan nöron sayısını rasgele olarak yarıya düşürüyor. Bu noktadan sonra sonucu gerçek ya da yalan olarak belirlenecek olan tek bir nörona sahip bir tane çıkış katmanı oluşturuluyor. Daha geniş bir açıdan baktığımız zaman 1 tane giriş katmanımız, 1 tane saklı katmanımız ve 1 tanede çıkış katmanımız oluşuyor. Şimdi bunun nasıl kodlandığına bakalım.\n"
      ]
    },
    {
      "metadata": {
        "id": "YKks-ZQ9eCuv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras import optimizers\n",
        "import datetime\n",
        "\n",
        "time = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
        "\n",
        "kerasboard = TensorBoard(log_dir=\"/tmp/tensorboard/{}\".format(time))\n",
        "\n",
        "#result = result.reshape((2777, 1, 2000))\n",
        "#testResult = testResult.reshape((489, 1, 2000))\n",
        "\n",
        "classifier = Sequential()\n",
        "\n",
        "classifier.add(Dense(1500, kernel_initializer=\"uniform\", activation='sigmoid', input_dim=3854)) #1 -> Giriş Katmanı\n",
        "#classifier.add(LSTM(1500, input_shape=(1, 2000), activation='sigmoid',return_sequences=False))\n",
        "classifier.add(Dropout(0.50))\n",
        "\n",
        "\n",
        "classifier.add(Dense(1500,kernel_initializer='uniform' ,activation='sigmoid')) #2 -> Saklı Katman\n",
        "classifier.add(Dropout(0.50))\n",
        "\n",
        "\n",
        "classifier.add(Dense(1, kernel_initializer=\"uniform\", activation='sigmoid')) #3 -> Çıkış Katmanı\n",
        "\n",
        "adam = optimizers.Adam(lr=0.0001, decay=0.00008)\n",
        "\n",
        "classifier.compile(optimizer=adam,loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "classifier.fit(result, targetInputFloat, epochs=40, callbacks=[kerasboard])\n",
        "\n",
        "prediction = classifier.predict(testResult)\n",
        "\n",
        "prediction = (prediction > 0.5)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "acScoreNN = accuracy_score(lb.fit_transform(testDataForComparison), prediction)\n",
        "print(\"Accuracy : \" + str(acScoreNN))\n",
        "\n",
        "probaNN = classifier.predict_proba(testResult)\n",
        "print(\"NN AUC \" + str(metrics.roc_auc_score(lb.fit_transform(testDataForComparison), probaNN)))\n",
        "\n",
        "print(\"tensorboard --logdir=\"+kerasboard.log_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PEt5LISjPRy8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard.notebook\n",
        "\n",
        "#Activate Tensorboard\n",
        "%tensorboard --logdir=/tmp/tensorboard/2019_03_30_10_34_18"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r1wi9YUKjftV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Saving the best model with Pickle (Neural %84.04)\n",
        "import pickle\n",
        "pickle.dump(classifier, open(\"NeuralNews\", 'wb'))\n",
        "\n",
        "loading = pickle.load(open(\"NeuralNews\", 'rb'))\n",
        "predictionPickleNeural = loading.predict(testResult)\n",
        "\n",
        "predictionPickleNeural = (predictionPickleNeural > 0.5)\n",
        "\n",
        "acScorePickleNeural = accuracy_score(lb.fit_transform(testDataForComparison), predictionPickleNeural)\n",
        "print(\"Accuracy Pickle Neural : \" + str(acScorePickleNeural))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}