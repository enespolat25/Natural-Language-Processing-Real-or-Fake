{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "News.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mertyyanik/Natural-Language-Processing-Real-or-Fake/blob/master/News.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "N0MgjiC5sUzr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Doğal Dil İşleme ve Derin Öğrenme - Gerçek ya da Sahte?\n",
        "Doğal Dil işleme (Natural Language Processing), basit hali ile yapay zeka ve dil biliminin birleşmesi ile ortaya çıkmış bir kavramdır. Bu kavram herkesin günlük konuşma dilinin bilgisayara öğretilmesi ve bazı çalışmaların bilgisayara yaptırılmasına sebep olmuştur. Günümüze yakın tarihlere baktığımız zaman bilgisayarların güçlenmesini takip eden makine öğrenmesi modelleri daha da gelişmiş ve bir çok alanda kullanılmaya başlanmıştır. Bu alanlardan biriside doğal dil işlemedir. Doğal dil işlemeyi makine öğrenmesi modelleri ile birleştirdiğimiz zaman ortaya bir cümlenin kime ait olduğunu ya da bir cümleyi söyleyen kişinin nasıl bir ruh haline sahip olduğunu anlayabilir noktaya gelmiş bulunmaktayız.\n",
        "\n",
        "Bizimde amacımız buna benzer bir çalışma yapmaktır. Elimizde haber sitelerinin yazmış olduğu haber başlıkları ve bu başlıkların gerçek ya da sahte olduğunu gösteren bir veri kümesi bulunmaktadır. Derin öğrenme yöntemlerini kullanarak bu veri kümesini bilgisayara öğretecek ve gelecekte yazılmış bir haber başlığının gerçek mi yoksa sahte mi olduğunu anlamaya çalışacağız."
      ]
    },
    {
      "metadata": {
        "id": "FIkAaBFktE21",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Bunun için Google'ın geliştirmiş olduğu Colab ortamını kullanıyoruz. Bu ortamın avantajı derin öğrenme yöntemlerinde uygulamaları eğitirken **TPU**'ları kullanıyor olmasıdır. TPU'lar, GPU veya CPU'ya göre 15 kat daha hızlıdır. Bu da bizi büyük bir oranda zaman maliyetinden kurtarmaktadır."
      ]
    },
    {
      "metadata": {
        "id": "Wb9SkSN9S5UF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Öncelikle elimizde ki veri kümelerini Colab ortamına aktarmamız gerekiyor. Bunu yapmadan önce ortamın Google Drive ile olan bağlantısını onaylamamız lazım. Aşağıda ki kod bloğunu çalıştırdığımız zaman ekranda bir link belirmektedir bu linke tıklayarak karşımıza çıkan kodu output kısmına yapıştırarak Google Drive ile olan bağlantıyı sağlamış oluyoruz."
      ]
    },
    {
      "metadata": {
        "id": "29Lt9X4PbrWI",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        " \n",
        "from pydrive.auth import GoogleAuth\n",
        " \n",
        "from pydrive.drive import GoogleDrive\n",
        " \n",
        "from google.colab import auth\n",
        " \n",
        "from oauth2client.client import GoogleCredentials\n",
        " \n",
        "auth.authenticate_user()\n",
        " \n",
        "gauth = GoogleAuth()\n",
        " \n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "itSyx3Sql4cA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Bundan sonraki aşama veri kümelerini Google Drive'dan indirmektir. Burada ki önemli nokta dosya numarasının (File ID) elde edilmesidir. Bu numaraya  Google Drive üzerinden veri kümesinin paylaş butonuna basarak ulaşabilirsiniz."
      ]
    },
    {
      "metadata": {
        "id": "Zb5eh3gMcXGh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Test.csv Dosyası\n",
        "from google.colab import files\n",
        "\n",
        "file_id = '1Zx7P0t9lQLP4aj4YOUaZDxNRrSkZOTk4'\n",
        "\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "\n",
        "downloaded.GetContentFile('test.csv')\n",
        "\n",
        "#Test.txt Dosyası\n",
        "from google.colab import files\n",
        " \n",
        "file_id = '1S2-N2HSIgtZpSzPmAEZWmVxV2YlOzCzs'\n",
        " \n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        " \n",
        "downloaded.GetContentFile('test.txt')\n",
        "\n",
        "#clean_real-Train.txt Dosyası\n",
        "from google.colab import files\n",
        " \n",
        "file_id = '1pnsf7izWzNYlA41xcWrCXQ5SSFe5JCUF'\n",
        " \n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        " \n",
        "downloaded.GetContentFile('clean_real-Train.txt')\n",
        "\n",
        "#clean_fake-Train.txt Dosyası\n",
        "from google.colab import files\n",
        " \n",
        "file_id = '1Mc1m6S6WCJuRkMBWM9_LZGHSzJIw2-e0'\n",
        " \n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        " \n",
        "downloaded.GetContentFile('clean_fake-Train.txt')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_Fe5yhr9m_eg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Bu noktadan sonra artık kendi Python kodlamanızı istediğiniz gibi yazabiliriz.\n",
        "\n",
        "Öncelikle gerekli kütüphaneleri yükleyelim."
      ]
    },
    {
      "metadata": {
        "id": "uyfureqInTdY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R0ZYcTFZMivK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Text dosyasında bulunan veri dosyalarımızı ayrı ayrı listelere aktarıyoruz. Böylece Python'ın nimetlerinden faydalanabileceğiz."
      ]
    },
    {
      "metadata": {
        "id": "y_8AGswcdRCv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Reading Data\n",
        "input1Data = open('clean_real-Train.txt', 'r+')\n",
        "input2Data = open('clean_fake-Train.txt', 'r+')\n",
        "testData = open('test.txt')\n",
        "\n",
        "testDataForComparison = pd.read_csv('test.csv').iloc[:, 1:]\n",
        "\n",
        "input1List = []\n",
        "input2List = []\n",
        "testList = []\n",
        "\n",
        "read = input1Data.readline()\n",
        "read2 = input2Data.readline()\n",
        "read3 = testData.readline()\n",
        "rowCount = 0\n",
        "while read != \"\":\n",
        "    input1List.append(read)\n",
        "    read = input1Data.readline()\n",
        "    \n",
        "while read2 != \"\":\n",
        "    input2List.append(read2)\n",
        "    read2 = input2Data.readline()\n",
        "\n",
        "while read3 != \"\":\n",
        "    testList.append(read3)\n",
        "    read3 = testData.readline()\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AM0hwXewMn0W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Daha sonra iki farklı veri kümesinde bulunan 'real' ve 'fake' veri kümelerini birleştiriyoruz. Real olanlara 1, fake olanlara 0 yazıyoruz."
      ]
    },
    {
      "metadata": {
        "id": "HtMraR2NhkjO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Real and Fake data combined.    \n",
        "liste = ['real' for i in range(len(input1List))]\n",
        "realInput = np.column_stack((input1List, liste))\n",
        "\n",
        "\n",
        "liste = ['fake' for i in range(len(input2List))]\n",
        "fakeInput = np.column_stack((input2List, liste))\n",
        "\n",
        "mainInput = realInput.tolist() + fakeInput.tolist()\n",
        "random.shuffle(mainInput)\n",
        "\n",
        "sentenceInput = []\n",
        "targetInput = []\n",
        "targetInputFloat = []\n",
        "for i in range(len(mainInput)):\n",
        "    sentenceInput.append(mainInput[i][0])\n",
        "    \n",
        "for i in range(len(mainInput)):\n",
        "    targetInput.append(mainInput[i][1])\n",
        "    \n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "lb = LabelEncoder()\n",
        "targetInputFloat = lb.fit_transform(targetInput)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3D3wvtrBNfFD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Veri Ön İşleme\n",
        "Bundan sonraki aşama veri ön işlemenin başladığı kısım. Veri kümesini bilgisayarın anlayabileceği sayılara çevirmeden önce bakıldığı zaman görülen bazı yanlışlar yok mu? Tabiki var. Bu yanlışların ne olduğundan bahsetmemiz ve en nihayetinde bunlardan kurtulmamız lazım. Yani veri kümemizi bazı veri ön işleme aşamalarından geçirmemiz gerekiyor. Bu aşamaları birkaç adımda sıralayabiliriz.\n",
        "1. Veri kümesi içerisinde noktalama işaretleri kelimelerin bir parçası olmadığı için veri kümesinden silinmelidir. İleride bahsedilecek ama biz temelinde kelimeleri 0 ve daha farklı sayılardan oluşan bir matris haline getirmeye çalışıyoruz. Burada eğer noktalama işaretlerini yok etmezsek bilgisayar bunları da bir sayı olarak düşünür ve yanıltıcı bir durum ortaya çıkar. Bu istediğimiz bir durum değil.\n",
        "2. İlk harfi büyük olan  Mehmet ile ilk harfi küçük olan mehmet farklı anlamamı gelmeli? Tabiki hayır. Bu nedenle elimizde ki veri kümesinde büyük harfleri küçük harflere çevirerek bu ayrımı ortadan kaldırmış oluyoruz.\n",
        "3. Tıpkı büyük harf küçük harf durumunda olduğu gibi Ahmet’in ya da Ahmet’e gibi kelimelerin birbirinden farklılığı olmaması gerekir. Bu nedenle bir kelimenin başına veya sonuna gelen bütün eklerden kurtulmuş olması gerekir. Yani kelimelerin sadece köklerden oluşmasını isteriz.\n",
        "4. Birleştirici Kelimeler. Bu durumunda diğerlerinden hiçbir farkı yok. ‘ve’, ‘veya’, ‘ile’ gibi bağlaçların silinmesi gerekir. Burada bütün adımlarda yaptığımız temel amaç aynıdır. Bu adımların sonunda geriye sadece kelimelerin kökleri kalmalıdır. Bunun dışında hiçbir şey istemiyoruz. Şimdi gelin bu işlerin Python kodlamasında nasıl yapıldığına bir bakalım.\n"
      ]
    },
    {
      "metadata": {
        "id": "ZecTmv8Bh_G6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Stop Words\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Stemmer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "# Regular Expressions\n",
        "import re\n",
        "\n",
        "sentenceList = []\n",
        "for i in range(len(sentenceInput)):\n",
        "    # Regular Expressions\n",
        "    comment = re.sub('[^a-zA-Z]', ' ', sentenceInput[i]) #-> 1.Adım\n",
        "    # Upper Case Problem\n",
        "    comment = comment.lower() #-> 2. Adım\n",
        "    # Splitting\n",
        "    comment = comment.split() #-> 3.Adım\n",
        "    # Stemming and Stop Words\n",
        "    comment = [ps.stem(kelime) for kelime in comment if not kelime in set(stopwords.words('english'))] # -> 4. Adım\n",
        "    \n",
        "    comment = ' '.join(comment)\n",
        "    sentenceList.append(comment)\n",
        "    \n",
        "temporaryResult = pd.DataFrame(data=sentenceList)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "estPIRqf_ed4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Burada ilk 3 adım gerekli kütüphanelerin çağrıldığı kısımdır. Bizi asıl ilgilendiren ‘for’ döngüsünün başladığı kısım. Bu döngünün amacı her cümleye tek tek erişmektir. Yani bu döngü her çalıştığında biz bir satırda ki cümleye ulaşıyoruz. Dolayısıyla o cümleye ulaştığımıza göre her şeyi tek bir döngü içerisinde yapabiliriz.\n",
        "\n",
        "**1.Adım(Regular Expressions) ->** İlk aşamada düzenli ifadeler(Regular Expressions) denilen bir kavram ortaya çıkıyor. Bu kavram bilgisayar bilimlerinde çok ayrıntılı bir şekilde anlatılan ve esasında bir programlama dili oluştururken o dile kurallar verdiğimiz kısımda kendini gösteriyor. Burada tek bilmemiz gereken şey 2 büyük parantez içerisinde ki ifadede a-z bütün küçük harflerin, A-Z bütün büyük ifadelerin, tırnak işaretinin bütün küçük harflerin ve bütün büyük harflerin dışında kalan ifadeleri temsil ediyor olmasıdır. İkinci kısımda ki boşluk karakteri, bu ifadelerden herhangi biriyle karşılaşılır ise o ifadeyi boşluk karakteri ile yer değiştirecektir. Bunu yaptığımız zaman bütün noktalama işaretlerinden kurtulmuş oluyoruz.\n",
        "\n",
        "**2.Adım(Upper Case Problem) ->** Hatırlarsanız ikinci adımımız büyük harflerden kurtulmaktı. Burada da ikinci adımda lower() fonksiyonunu kullanarak bütün büyük harfleri küçük harfler ile yer değiştirmiş olduk.\n",
        "\n",
        "**3.Adım(Splitting) ->** Daha sonrasında yapmamız gereken şey kelime eklerinden ve bağlaç gibi cümleden bağımsız olan kelimelerden kurtulmak. Bunu yapmadan önce elimizde her bir kelimenin olduğu bir liste olması gerekiyor. Split() fonksiyonu cümledeki her bir kelimeyi bir listeye aktarıyor. Şimdi artık bu kelimelerin her birine tek tek ulaşarak o kelimenin üzerinde işlem yapabiliriz.\n",
        "\n",
        "**4.Adım(Stemming and Stop Words) ->** Bir alt satırda ki kod parçasında bir döngü ile her bir kelimeye tek tek ulaşıyor ve eğer bu kelime bir stopwords değilse o zaman stem() fonksiyonu ile başında veya sonunda bulunan eklerden kurtulmasını sağlıyoruz. Eğer bir stopwords ise o zaman kelimeyi tamamen yok ediyoruz.\n",
        "Bütün bu adımların sonunda artık elimizde o 4 adımda istediğimiz sadece köklerden oluşan bir veri kümesi olması gerekiyor. Bakalım gerçekten öyle olmuş mu?"
      ]
    },
    {
      "metadata": {
        "id": "dOebcgsp_pCM",
        "colab_type": "code",
        "outputId": "79f274ce-bcb4-4da7-fb63-cb1279ccc7e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1092
        }
      },
      "cell_type": "code",
      "source": [
        "print(temporaryResult)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                      0\n",
            "0                    trump famili say trump brand damag\n",
            "1     donald trump unannounc visit korean dmz abando...\n",
            "2            watch loui ck epic answer trump vs hillari\n",
            "3               trump ask common daughter answer creepi\n",
            "4            presid elect trump new era unpredict await\n",
            "5                             trump busi indonesia make\n",
            "6               constanc hall judgey women donald trump\n",
            "7                trump leak truth power war win analysi\n",
            "8     donald trump presid news australian economi de...\n",
            "9             kevin macdonald celebr trump amaz victori\n",
            "10    sur twitter donald trump se f licit avoir vot ...\n",
            "11                     concern rais press freedom trump\n",
            "12                   investor welcom donald trump worri\n",
            "13    berni sander could replac presid trump littl k...\n",
            "14    us women open golf may yet move donald trump o...\n",
            "15                  donald trump janet yellen head head\n",
            "16    donald trump muslim ban immedi consequ zoe daniel\n",
            "17                    trump tell crowd texa handl anyth\n",
            "18    netanyahu scuttl trump russian rapproch new ea...\n",
            "19                    john mccain say donald trump make\n",
            "20                  specul trump speak one china polici\n",
            "21              donald trump speech congress key moment\n",
            "22    donald trump sanction venezuela vice presid dr...\n",
            "23                        turnbul trump meet say spicer\n",
            "24     break trump take lead battleground state florida\n",
            "25    donald trump take australia unchart water pete...\n",
            "26                   trump assail gm car product mexico\n",
            "27           hasan minhaj roast trump correspond dinner\n",
            "28         trump take aim republican conserv must fight\n",
            "29       trump pick mulvaney white hous budget director\n",
            "...                                                 ...\n",
            "2747  barri cassidi trump victori make global wave a...\n",
            "2748           black church burn spray paint vote trump\n",
            "2749                   donald trump go grab world p ssi\n",
            "2750                donald trump job number sean spicer\n",
            "2751                  delet account episod presid trump\n",
            "2752                   trump warn israel new settlement\n",
            "2753                man behind trump ralli disturb reno\n",
            "2754  fantast trump point plan reform healthcar begi...\n",
            "2755      donald trump say relationship russia time low\n",
            "2756                                              trump\n",
            "2757    donald trump tweet presidenti record delet edit\n",
            "2758  war break neo con libertarian trump foreign po...\n",
            "2759  donald trump mix messag malcolm turnbul phone ...\n",
            "2760      donald trump promis pardon snowden assang man\n",
            "2761  trump surg tie clinton michigan fbi wipe hilla...\n",
            "2762  judgment day one reason everi christian jew am...\n",
            "2763      julian assang say trump wont allow win inform\n",
            "2764                      trump amaz victori stack deck\n",
            "2765  donald trump trade insult republican senat bob...\n",
            "2766  reward clinton hawkish trump foreign polici un...\n",
            "2767                 donald trump begin asia trip japan\n",
            "2768                            trumpism repres languag\n",
            "2769  donald trump disabl report whole truth spin pu...\n",
            "2770   jame comey take aim donald trump senat testimoni\n",
            "2771               trump commit us japan secur abe meet\n",
            "2772  dillari trumplon guy epic rap song destroy tru...\n",
            "2773     report donald trump call mexico stop bad hombr\n",
            "2774                proof iowa cop killer trump support\n",
            "2775  north korea trump condemn rogu nation hostil a...\n",
            "2776  comment donald trump ex wife ivana say make ch...\n",
            "\n",
            "[2777 rows x 1 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "THi3jm8-PXaQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Evet, gerçekten veri kümesi içerisinde sadece köklerin olduğunu ve herhangi bir bağlaç kalmadığını görebiliyoruz. Tam olarak istediğimiz durum buydu. Artık veri kümesini bilgisayarın anlayabileceği 0 ve 1 lerden oluşan bir matris haline getirebiliriz.\n",
        "\n",
        "Bunun için kullandığımız Python kütüphanesi Scikit-Learn. Bu kütüphanenin içerisindeki CountVectorizer sınıfını kullanarak bu işlemi yapabiliyoruz. Bu sınıf veri kümesinde ki bütün kelimeleri alıp birer değişken haline getiriyor ve her bir cümlede o kelimenin olup olmadığına göre 0 ve 1 sayısını veriyor. En nihayetinde elimizde çok fazla sayıda 0 ve çok az sayıda 1’lerden oluşan bir sparse matris oluşuyor.\n"
      ]
    },
    {
      "metadata": {
        "id": "0NLkNMT1u-sX",
        "colab_type": "code",
        "outputId": "62358994-8416-4d41-bff0-0b6621583863",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2150
        }
      },
      "cell_type": "code",
      "source": [
        "# Vectorized All Data\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(sentenceList)\n",
        "\n",
        "result = pd.DataFrame(data=X.toarray())\n",
        "\n",
        "print(result)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      0     1     2     3     4     5     6     7     8     9     ...   3844  \\\n",
            "0        0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "1        0     1     0     0     0     0     0     0     0     0  ...      0   \n",
            "2        0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "3        0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "4        0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "5        0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "6        0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "7        0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "8        0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "9        0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "10       0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "11       0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "12       0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "13       0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "14       0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "15       0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "16       0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "17       0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "18       0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "19       0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "20       0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "21       0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "22       0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "23       0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "24       0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "25       0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "26       0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "27       0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "28       0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "29       0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...    ...   \n",
            "2747     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "2748     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "2749     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "2750     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "2751     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "2752     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "2753     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "2754     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "2755     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "2756     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "2757     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "2758     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "2759     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "2760     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "2761     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "2762     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "2763     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "2764     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "2765     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "2766     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "2767     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "2768     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "2769     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "2770     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "2771     0     0     0     0     0     1     0     0     0     0  ...      0   \n",
            "2772     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "2773     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "2774     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "2775     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "2776     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
            "\n",
            "      3845  3846  3847  3848  3849  3850  3851  3852  3853  \n",
            "0        0     0     0     0     0     0     0     0     0  \n",
            "1        0     0     0     0     0     0     0     0     0  \n",
            "2        0     0     0     0     0     0     0     0     0  \n",
            "3        0     0     0     0     0     0     0     0     0  \n",
            "4        0     0     0     0     0     0     0     0     0  \n",
            "5        0     0     0     0     0     0     0     0     0  \n",
            "6        0     0     0     0     0     0     0     0     0  \n",
            "7        0     0     0     0     0     0     0     0     0  \n",
            "8        0     0     0     0     0     0     0     0     0  \n",
            "9        0     0     0     0     0     0     0     0     0  \n",
            "10       0     0     0     0     0     0     0     0     0  \n",
            "11       0     0     0     0     0     0     0     0     0  \n",
            "12       0     0     0     0     0     0     0     0     0  \n",
            "13       0     0     0     0     0     0     0     0     0  \n",
            "14       0     0     0     0     0     0     0     0     0  \n",
            "15       0     0     0     0     0     0     0     0     0  \n",
            "16       0     0     0     1     0     0     0     0     0  \n",
            "17       0     0     0     0     0     0     0     0     0  \n",
            "18       0     0     0     0     0     0     0     0     0  \n",
            "19       0     0     0     0     0     0     0     0     0  \n",
            "20       0     0     0     0     0     0     0     0     0  \n",
            "21       0     0     0     0     0     0     0     0     0  \n",
            "22       0     0     0     0     0     0     0     0     0  \n",
            "23       0     0     0     0     0     0     0     0     0  \n",
            "24       0     0     0     0     0     0     0     0     0  \n",
            "25       0     0     0     0     0     0     0     0     0  \n",
            "26       0     0     0     0     0     0     0     0     0  \n",
            "27       0     0     0     0     0     0     0     0     0  \n",
            "28       0     0     0     0     0     0     0     0     0  \n",
            "29       0     0     0     0     0     0     0     0     0  \n",
            "...    ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
            "2747     0     0     0     0     0     0     0     0     0  \n",
            "2748     0     0     0     0     0     0     0     0     0  \n",
            "2749     0     0     0     0     0     0     0     0     0  \n",
            "2750     0     0     0     0     0     0     0     0     0  \n",
            "2751     0     0     0     0     0     0     0     0     0  \n",
            "2752     0     0     0     0     0     0     0     0     0  \n",
            "2753     0     0     0     0     0     0     0     0     0  \n",
            "2754     0     0     0     0     0     0     0     0     0  \n",
            "2755     0     0     0     0     0     0     0     0     0  \n",
            "2756     0     0     0     0     0     0     0     0     0  \n",
            "2757     0     0     0     0     0     0     0     0     0  \n",
            "2758     0     0     0     0     0     0     0     0     0  \n",
            "2759     0     0     0     0     0     0     0     0     0  \n",
            "2760     0     0     0     0     0     0     0     0     0  \n",
            "2761     0     0     0     0     0     0     0     0     0  \n",
            "2762     0     0     0     0     0     0     0     0     0  \n",
            "2763     0     0     0     0     0     0     0     0     0  \n",
            "2764     0     0     0     0     0     0     0     0     0  \n",
            "2765     0     0     0     0     0     0     0     0     0  \n",
            "2766     0     0     0     0     0     0     0     0     0  \n",
            "2767     0     0     0     0     0     0     0     0     0  \n",
            "2768     0     0     0     0     0     0     0     0     0  \n",
            "2769     0     0     0     0     0     0     0     0     0  \n",
            "2770     0     0     0     0     0     0     0     0     0  \n",
            "2771     0     0     0     0     0     0     0     0     0  \n",
            "2772     0     0     0     0     0     0     0     0     0  \n",
            "2773     0     0     0     0     0     0     0     0     0  \n",
            "2774     0     0     0     0     0     0     0     0     0  \n",
            "2775     0     0     0     0     0     0     0     0     0  \n",
            "2776     0     0     1     0     0     0     0     0     0  \n",
            "\n",
            "[2777 rows x 3854 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zvDlR5bLQA6I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Şu noktadan itibaren bu veri kümesi artık bilgisayarın anlayabileceği bir model haline geldi."
      ]
    },
    {
      "metadata": {
        "id": "JxVOM_dEPzH5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Test veri kümemiz ile eğitim veri kümemiz birbirinden tamamen ayrı. Bu nedenle bu işlemlerin birebir aynısı test veri kümesi içinde uygulanmıştır.\n"
      ]
    },
    {
      "metadata": {
        "id": "XUGxSi7DdrQg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Test Data\n",
        "testSentenceList = []\n",
        "for i in range(len(testList)):\n",
        "    # Regular Expressions\n",
        "    comment = re.sub('[^a-zA-Z]', ' ', testList[i])\n",
        "    # Upper Case Problem\n",
        "    comment = comment.lower()\n",
        "    # Splitting\n",
        "    comment = comment.split()\n",
        "    # Stemming and Stop Words\n",
        "    comment = [ps.stem(kelime) for kelime in comment if not kelime in set(stopwords.words('english'))]\n",
        "    \n",
        "    comment = ' '.join(comment)\n",
        "    testSentenceList.append(comment)\n",
        "\n",
        "X = vectorizer.transform(testSentenceList)\n",
        "\n",
        "testResult = pd.DataFrame(data=X.toarray())\n",
        "# PCA for Test\n",
        "#testResult = pca.transform(testResult)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xrVkk9uuQnlj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Derin Öğrenme\n",
        "Nihayet artık veri kümesi bir model oluşturmaya hazır hale gelmiştir. Modelimiz bir derin öğrenme modelidir. Dolayısıyla derin öğrenmenin ne olduğundan kısaca bahsetmekte fayda var. Derin öğrenme, basit yapay sinir ağlarının daha karmaşık hale getirilmişi ve her bir katmana istediğimiz gibi müdahale edebildiğimiz bir yapısıdır. Karmaşıklık arttıkça derin öğrenmeye olan ihtiyaçta artıyor.\n"
      ]
    },
    {
      "metadata": {
        "id": "SmbmZpvrSKyX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "[![image](https://i.hizliresim.com/9a0AGO.jpg)](https://hizliresim.com/9a0AGO)"
      ]
    },
    {
      "metadata": {
        "id": "rGjMpTxjTayP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Şimdi nasıl bir model tasarladığıma kısaca bir bakalım. Elimizde 1 tane giriş katmanı, 1 tane saklı katman ve 2 tane seyreltme katmanı bulunmaktadır. Giriş ve saklı katman 1500’er tane nörona sahiptir. 2 tane seyreltme katmanımız var ve bu katmanlar 0.5 eşik değerine sahip. Bu seyreltme katmanlarının temel amacı ezberlemeyi azaltmaktır. Bu istemediğimiz bir durum. Dolayısıyla bu seyreltme katmanları eşik değerini 0.5 olarak girdiğimiz için burada 1500 tane olan nöron sayısını rasgele olarak yarıya düşürüyor. Bu noktadan sonra sonucu gerçek ya da yalan olarak belirlenecek olan tek bir nörona sahip bir tane çıkış katmanı oluşturuluyor. Daha geniş bir açıdan baktığımız zaman 1 tane giriş katmanımız, 1 tane saklı katmanımız ve 1 tanede çıkış katmanımız oluşuyor. Şimdi bunun nasıl kodlandığına bakalım.\n"
      ]
    },
    {
      "metadata": {
        "id": "YKks-ZQ9eCuv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras import optimizers\n",
        "import datetime\n",
        "\n",
        "time = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
        "\n",
        "kerasboard = TensorBoard(log_dir=\"/tmp/tensorboard/{}\".format(time))\n",
        "\n",
        "#result = result.reshape((2777, 1, 2000))\n",
        "#testResult = testResult.reshape((489, 1, 2000))\n",
        "\n",
        "classifier = Sequential()\n",
        "\n",
        "classifier.add(Dense(1500, kernel_initializer=\"glorot_normal\", activation='sigmoid', input_dim=3854)) #1 -> Giriş Katmanı\n",
        "#classifier.add(LSTM(1500, input_shape=(1, 2000), activation='sigmoid',return_sequences=False))\n",
        "classifier.add(Dropout(0.50))\n",
        "\n",
        "\n",
        "classifier.add(Dense(1500,kernel_initializer='glorot_normal' ,activation='sigmoid')) #2 -> Saklı Katman\n",
        "classifier.add(Dropout(0.50))\n",
        "\n",
        "\n",
        "classifier.add(Dense(1, kernel_initializer=\"glorot_normal\", activation='sigmoid')) #3 -> Çıkış Katmanı\n",
        "\n",
        "adam = optimizers.Adam(lr=0.0001, decay=0.00008)\n",
        "\n",
        "classifier.compile(optimizer=adam,loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c-nYSEmFB-Hs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Öncelikle kullanılan kütüphane Google’ın bir çalışanı tarafından tasarlanan Keras kütüphanesidir. Bu kütüphane arka planında Tensorflow denen başka bir kütüphaneyi kullanmaktadır. Bu kütüphaneyi ise Google şirketinin direk kendisi tasarladı. Bu iki kütüphane sayesinde Python üzerinden Derin öğrenme modelleri oluşturabiliyoruz. Bunlardan başka kütüphanelerde vardır fakat en çok kullanılan iki kütüphane bu ikisidir.\n",
        "\n",
        "Burada odaklanmamız gereken kısımlar Dense ve Dropout fonksiyonlarının olduğu kısımlardır. \n",
        "\n",
        "**Giriş Katmanı -> ** 1 numaralı kısımda ilk parametre olarak 1500 sayısını girerek nöron sayısını girmiş olduk. \n",
        "\n",
        "İkinci parametre kernel_initializer başlangıçta hangi istatistiksel dağılımı kullanarak nöronlara ağırlıklar vereceğimizi belirlediğimiz kısım. Ağırlık verirken dikkat edilmesi gereken 3 tane husus vardır.\n",
        "1. Her bir optimizasyon gerçekleştiğinde ağırlıkların ortalamasının sıfıra eşit olması gerekir.\n",
        "2. Bütün ağırlıkların varyanslarının eşit olması gerekmektedir yani değişen varyanslılık sorununun olmamasıdır.\n",
        "3. Ağırlıkların kesinlikle sıfıra eşit olmaması ve ne çok küçük değerler ne de çok büyük değerlere eşit olmasıdır.\n",
        "\n",
        "Bütün bu hususların ışığında en çok kullanılan 2 tane initializer Xavier ve He'dir. Ben çalışmamda Xavier(glorot_normal) kullandım.\n",
        "\n",
        "Diğer bir parametre aktivasyon fonksiyonudur, bu fonksiyon girişteki katsayıyı alıp kendi fonksiyonunda işlem uygulayarak çıktı katsayısı elde etmemizi sağlıyor. Daha sonra bu çıktı katsayısı ya direk sonucun kendisi olur ya da bir başka katmanın girdisi olur.\n",
        "Aktivasyon fonksiyonlarının 2 tane ana özelliği vardır.\n",
        "1. Derin Öğrenme kullanırken elimizdeki veri setinin karmaşıklığı o veri setinin çok büyük bir ihtimalle doğrusal olmayacağını ön görür. Bu sebeple bu fonksiyonlar doğrusal olmayan fonksiyonlardır.\n",
        "2. Her bir geri beslemede bu aktivasyon fonksiyonlarının türevi alınarak en optimal sonuca ulaşmak amaçlanır. Bu nedenle kolay türevi alınabilen fonksiyonlar tercih edilir böylece zaten sistemi çok  yoran derin öğrenme algoritmalarının iş yükü biraz daha hafifletilmiş olur.\n",
        "\n",
        "Burada aktivasyon fonksiyonu olarak en popüleri olan **Sigmoid** fonksiyonu kullanmayı tercih ettim.\n",
        "\n",
        "En son parametre olarakta elimizdeki veri kümesinin değişken sayısını vermemiz gerekiyor. Bu bilgi veri kümesindeki kelime sayısına eşittir yani 3854.\n",
        "\n",
        "Daha sonra Dropout katmanını oluşturuyoruz ve eşik değeri olarak 0.5 giriyoruz. \n",
        "\n",
        "**Saklı Katman -> ** 2 numaralı kısımda 1 numaralı kısımda yaptığımız şeyden farklı olan hiçbir şey yapmıyoruz. \n",
        "\n",
        "**Çıkış Katmanı -> ** 3 numaralı kısımda ise sonucu doğru ya da yalan olan 1 tane nörona sahip bir katman oluşturuyoruz.\n",
        "\n",
        "Bundan sonraki aşamada bir optimizasyon fonksiyonu belirlememiz gerekiyor. Bu fonksiyonun amacı ise her bir geri beslemede nöronların katsayılarını daha optimal hale getirip doğruluk yüzdemizi arttırmayı amaçlamak. Bakalım kodlarımız nasıl çalışmış."
      ]
    },
    {
      "metadata": {
        "id": "3EX_pA1KCJoD",
        "colab_type": "code",
        "outputId": "977fa02f-84a6-4242-b739-4b9eb473a813",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1378
        }
      },
      "cell_type": "code",
      "source": [
        "classifier.fit(result, targetInputFloat, epochs=40, callbacks=[kerasboard])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "2777/2777 [==============================] - 16s 6ms/step - loss: 0.7630 - acc: 0.5290\n",
            "Epoch 2/40\n",
            "2777/2777 [==============================] - 15s 6ms/step - loss: 0.7226 - acc: 0.5574\n",
            "Epoch 3/40\n",
            "2777/2777 [==============================] - 15s 5ms/step - loss: 0.7232 - acc: 0.5582\n",
            "Epoch 4/40\n",
            "2777/2777 [==============================] - 15s 5ms/step - loss: 0.6978 - acc: 0.5834\n",
            "Epoch 5/40\n",
            "2777/2777 [==============================] - 15s 5ms/step - loss: 0.6855 - acc: 0.5870\n",
            "Epoch 6/40\n",
            "2777/2777 [==============================] - 15s 6ms/step - loss: 0.6614 - acc: 0.6169\n",
            "Epoch 7/40\n",
            "2777/2777 [==============================] - 15s 6ms/step - loss: 0.6643 - acc: 0.6060\n",
            "Epoch 8/40\n",
            "2777/2777 [==============================] - 16s 6ms/step - loss: 0.6465 - acc: 0.6097\n",
            "Epoch 9/40\n",
            "2777/2777 [==============================] - 15s 5ms/step - loss: 0.6298 - acc: 0.6421\n",
            "Epoch 10/40\n",
            "2777/2777 [==============================] - 15s 5ms/step - loss: 0.6143 - acc: 0.6485\n",
            "Epoch 11/40\n",
            "2777/2777 [==============================] - 15s 5ms/step - loss: 0.5889 - acc: 0.6986\n",
            "Epoch 12/40\n",
            "2777/2777 [==============================] - 15s 5ms/step - loss: 0.5644 - acc: 0.7148\n",
            "Epoch 13/40\n",
            "2777/2777 [==============================] - 15s 6ms/step - loss: 0.5455 - acc: 0.7245\n",
            "Epoch 14/40\n",
            "2777/2777 [==============================] - 15s 6ms/step - loss: 0.5175 - acc: 0.7559\n",
            "Epoch 15/40\n",
            "2777/2777 [==============================] - 15s 5ms/step - loss: 0.5052 - acc: 0.7555\n",
            "Epoch 16/40\n",
            "2777/2777 [==============================] - 15s 5ms/step - loss: 0.4608 - acc: 0.7850\n",
            "Epoch 17/40\n",
            "2777/2777 [==============================] - 15s 5ms/step - loss: 0.4566 - acc: 0.7926\n",
            "Epoch 18/40\n",
            "2777/2777 [==============================] - 15s 5ms/step - loss: 0.4403 - acc: 0.7947\n",
            "Epoch 19/40\n",
            "2777/2777 [==============================] - 15s 5ms/step - loss: 0.4264 - acc: 0.8063\n",
            "Epoch 20/40\n",
            "2777/2777 [==============================] - 15s 5ms/step - loss: 0.4143 - acc: 0.8149\n",
            "Epoch 21/40\n",
            "2777/2777 [==============================] - 15s 5ms/step - loss: 0.3981 - acc: 0.8203\n",
            "Epoch 22/40\n",
            "2777/2777 [==============================] - 15s 6ms/step - loss: 0.3828 - acc: 0.8250\n",
            "Epoch 23/40\n",
            "2777/2777 [==============================] - 15s 6ms/step - loss: 0.3736 - acc: 0.8369\n",
            "Epoch 24/40\n",
            "2777/2777 [==============================] - 15s 5ms/step - loss: 0.3600 - acc: 0.8344\n",
            "Epoch 25/40\n",
            "2777/2777 [==============================] - 15s 5ms/step - loss: 0.3518 - acc: 0.8430\n",
            "Epoch 26/40\n",
            "2777/2777 [==============================] - 15s 5ms/step - loss: 0.3460 - acc: 0.8372\n",
            "Epoch 27/40\n",
            "2777/2777 [==============================] - 15s 5ms/step - loss: 0.3376 - acc: 0.8509\n",
            "Epoch 28/40\n",
            "2777/2777 [==============================] - 15s 6ms/step - loss: 0.3425 - acc: 0.8473\n",
            "Epoch 29/40\n",
            "2777/2777 [==============================] - 15s 5ms/step - loss: 0.3108 - acc: 0.8610\n",
            "Epoch 30/40\n",
            "2777/2777 [==============================] - 15s 5ms/step - loss: 0.3195 - acc: 0.8588\n",
            "Epoch 31/40\n",
            "2777/2777 [==============================] - 15s 5ms/step - loss: 0.3071 - acc: 0.8682\n",
            "Epoch 32/40\n",
            "2777/2777 [==============================] - 15s 5ms/step - loss: 0.3072 - acc: 0.8671\n",
            "Epoch 33/40\n",
            "2777/2777 [==============================] - 15s 5ms/step - loss: 0.3117 - acc: 0.8653\n",
            "Epoch 34/40\n",
            "2777/2777 [==============================] - 15s 5ms/step - loss: 0.2929 - acc: 0.8682\n",
            "Epoch 35/40\n",
            "2777/2777 [==============================] - 15s 5ms/step - loss: 0.2920 - acc: 0.8689\n",
            "Epoch 36/40\n",
            "2777/2777 [==============================] - 15s 5ms/step - loss: 0.2850 - acc: 0.8743\n",
            "Epoch 37/40\n",
            "2777/2777 [==============================] - 15s 5ms/step - loss: 0.2881 - acc: 0.8797\n",
            "Epoch 38/40\n",
            "2777/2777 [==============================] - 15s 5ms/step - loss: 0.2698 - acc: 0.8797\n",
            "Epoch 39/40\n",
            "2777/2777 [==============================] - 15s 5ms/step - loss: 0.2659 - acc: 0.8830\n",
            "Epoch 40/40\n",
            "2777/2777 [==============================] - 15s 5ms/step - loss: 0.2535 - acc: 0.8949\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fba0665c908>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "IaaQpqwzKcCL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Modellemede toplamda 40 tane geri besleme yani optimizasyon çalışması yapıldı. Loss yani hata değerlerinin giderek düşmesini bekliyoruz gerçekten de öyle olmuş. Accuracy yani doğruluk yüzdelerinin de her bir geri beslemede artmasını amaçlıyoruz. Oda artmış, ilk başta %52 doğru tahmin yapabilirken 40. geri beslemede %89 oranına çıkmış. Bu oranların hepsi eğitim veri kümesi için, bakalım modele test veri kümesini verdiğimizde nasıl bir sonuç alacağız en nihayetinde bizim için asıl önemli olan kısım modelin daha önce bilmediği bir cümleyi nasıl analiz ettiği.\n"
      ]
    },
    {
      "metadata": {
        "id": "lUmHwukhKvC1",
        "colab_type": "code",
        "outputId": "fda0e906-c0de-4701-ee0d-c44abde545c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "cell_type": "code",
      "source": [
        "prediction = classifier.predict(testResult)\n",
        "\n",
        "prediction = (prediction > 0.5)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "acScoreNN = accuracy_score(lb.fit_transform(testDataForComparison), prediction)\n",
        "print(\"Accuracy : \" + str(acScoreNN))\n",
        "\n",
        "probaNN = classifier.predict_proba(testResult)\n",
        "print(\"NN AUC \" + str(metrics.roc_auc_score(lb.fit_transform(testDataForComparison), probaNN)))\n",
        "\n",
        "print(\"tensorboard --logdir=\"+kerasboard.log_dir)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy : 0.8220858895705522\n",
            "NN AUC 0.9125109208457103\n",
            "tensorboard --logdir=/tmp/tensorboard/2019_04_21_18_28_31\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "u3ty-SzJK3uu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Sonuçlarda görüldüğü gibi model daha önce bilmediği bir veri kümesiyle karşılaştığı zaman %83 doğru tahmin yüzdesine ulaşmış.\n",
        "\n",
        "ROC Eğrisi altında kalan alan %91 çıkmış. Bundan çıkan yorum modelin %91 oranında doğruyla yanlışı ayırt edebilme gücüne sahip olduğudur. Bu değer 1’e ne kadar yakın olursa model o kadar iyi demektir. Tabi ROC Eğrisi sonucunun doğruluk yüzdesine göre daha yanıltıcı sonuç verebileceğini söylemekte fayda var.\n"
      ]
    },
    {
      "metadata": {
        "id": "yQ7r6NiSNA2q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Şimdi oluşturmuş olduğumuz derin öğrenme modelinin her bir geri beslemede doğruluk yüzdesini nasıl beslediğini TensorBoard ortamında görselleştirelim."
      ]
    },
    {
      "metadata": {
        "id": "PEt5LISjPRy8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard.notebook\n",
        "\n",
        "#Activate Tensorboard\n",
        "%tensorboard --logdir=/tmp/tensorboard/2019_04_21_18_28_31"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9pWrToPbNi18",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TensorBoard'u çağırmak için derin öğrenme modelini kurarken oluşturduğumuz log adresini kullanıyoruz. Normal şartlarda bu log adresinin farklı bir tarayıcı sekmesinde açılmasını bekleriz fakat Jupyter Notebook'a yeni gelmiş bir eklenti sayesinde bunu artık kod hücresinin içerisinde açabiliyoruz. (tensorboard.notebook eklentisi)"
      ]
    },
    {
      "metadata": {
        "id": "yYCRmiG1OnUS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Bundan sonraki aşamada oluşturmuş olduğumuz derin öğrenme modelini Pickle kütüphanesini kullanarak kaydediyoruz. Böylece ne zaman istersek model üzerinden tahminler gerçekleştirebileceğiz."
      ]
    },
    {
      "metadata": {
        "id": "r1wi9YUKjftV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Saving the best model with Pickle (Neural %84.04)\n",
        "import pickle\n",
        "pickle.dump(classifier, open(\"NeuralNews\", 'wb'))\n",
        "\n",
        "loading = pickle.load(open(\"NeuralNews\", 'rb'))\n",
        "predictionPickleNeural = loading.predict(testResult)\n",
        "\n",
        "predictionPickleNeural = (predictionPickleNeural > 0.5)\n",
        "\n",
        "acScorePickleNeural = accuracy_score(lb.fit_transform(testDataForComparison), predictionPickleNeural)\n",
        "print(\"Accuracy Pickle Neural : \" + str(acScorePickleNeural))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}