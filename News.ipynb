{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "News.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mertyyanik/Natural-Language-Processing-Real-or-Fake/blob/master/News.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "N0MgjiC5sUzr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Doğal Dil İşleme ve Derin Öğrenme - Gerçek ya da Sahte?\n",
        "Doğal Dil işleme (Natural Language Processing), basit hali ile yapay zeka ve dil biliminin birleşmesi ile ortaya çıkmış bir kavramdır. Bu kavram herkesin günlük konuşma dilinin bilgisayara öğretilmesi ve bazı çalışmaların bilgisayara yaptırılmasına sebep olmuştur. Günümüze yakın tarihlere baktığımız zaman bilgisayarların güçlenmesini takip eden makine öğrenmesi modelleri daha da gelişmiş ve bir çok alanda kullanılmaya başlanmıştır. Bu alanlardan biriside doğal dil işlemedir. Doğal dil işlemeyi makine öğrenmesi modelleri ile birleştirdiğimiz zaman ortaya bir cümlenin kime ait olduğunu ya da bir cümleyi söyleyen kişinin nasıl bir ruh haline sahip olduğunu anlayabilir noktaya gelmiş bulunmaktayız.\n",
        "\n",
        "Bizimde amacımız buna benzer bir çalışma yapmaktır. Elimizde haber sitelerinin yazmış olduğu haber başlıkları ve bu başlıkların gerçek ya da sahte olduğunu gösteren bir veri kümesi bulunmaktadır. Derin öğrenme yöntemlerini kullanarak bu veri kümesini bilgisayara öğretecek ve gelecekte yazılmış bir haber başlığının gerçek mi yoksa sahte mi olduğunu anlamaya çalışacağız."
      ]
    },
    {
      "metadata": {
        "id": "FIkAaBFktE21",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Bunun için Google'ın geliştirmiş olduğu Colab ortamını kullanıyoruz. Bu ortamın avantajı derin öğrenme yöntemlerinde modelleri eğitirken **TPU**'ları kullanıyor olmasıdır. Google üreticilerine göre TPU'lar, GPU veya CPU'ya göre 15 kat daha hızlıdır. Bu da bizi büyük bir oranda zaman maliyetinden kurtarmaktadır."
      ]
    },
    {
      "metadata": {
        "id": "Wb9SkSN9S5UF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Öncelikle elimizde ki veri kümelerini Colab ortamına aktarmamız gerekiyor. Bunu yapmadan önce ortamın Google Drive ile olan bağlantısını onaylamamız lazım. Aşağıda ki kod bloğunu çalıştırdığımız zaman ekranda bir link belirmektedir bu linke tıklayarak karşımıza çıkan kodu output kısmına yapıştırarak Google Drive ile olan bağlantıyı sağlamış oluyoruz."
      ]
    },
    {
      "metadata": {
        "id": "29Lt9X4PbrWI",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        " \n",
        "from pydrive.auth import GoogleAuth\n",
        " \n",
        "from pydrive.drive import GoogleDrive\n",
        " \n",
        "from google.colab import auth\n",
        " \n",
        "from oauth2client.client import GoogleCredentials\n",
        " \n",
        "auth.authenticate_user()\n",
        " \n",
        "gauth = GoogleAuth()\n",
        " \n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "itSyx3Sql4cA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Bundan sonraki aşama veri kümelerini Google Drive'dan indirmektir. Burada ki önemli nokta dosya numarasının (File ID) elde edilmesidir. Bu numaraya  Google Drive üzerinden veri kümesinin paylaş butonuna basarak ulaşabilirsiniz."
      ]
    },
    {
      "metadata": {
        "id": "Zb5eh3gMcXGh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "#Test.csv Dosyası\n",
        "\n",
        "file_id = '1Zx7P0t9lQLP4aj4YOUaZDxNRrSkZOTk4'\n",
        "\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "\n",
        "downloaded.GetContentFile('test.csv')\n",
        "\n",
        "#Test.txt Dosyası\n",
        " \n",
        "file_id = '1S2-N2HSIgtZpSzPmAEZWmVxV2YlOzCzs'\n",
        " \n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        " \n",
        "downloaded.GetContentFile('test.txt')\n",
        "\n",
        "#clean_real-Train.txt Dosyası\n",
        " \n",
        "file_id = '1pnsf7izWzNYlA41xcWrCXQ5SSFe5JCUF'\n",
        " \n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        " \n",
        "downloaded.GetContentFile('clean_real-Train.txt')\n",
        "\n",
        "#clean_fake-Train.txt Dosyası\n",
        " \n",
        "file_id = '1Mc1m6S6WCJuRkMBWM9_LZGHSzJIw2-e0'\n",
        " \n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        " \n",
        "downloaded.GetContentFile('clean_fake-Train.txt')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_Fe5yhr9m_eg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Bu noktadan sonra artık kendi Python kodlarımızı istediğimiz gibi yazabiliriz.\n",
        "\n",
        "Öncelikle gerekli kütüphaneleri yükleyelim."
      ]
    },
    {
      "metadata": {
        "id": "uyfureqInTdY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R0ZYcTFZMivK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Text dosyasında bulunan veri dosyalarımızı ayrı ayrı listelere aktarıyoruz. Böylece Python'ın nimetlerinden faydalanabileceğiz."
      ]
    },
    {
      "metadata": {
        "id": "y_8AGswcdRCv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Reading Data\n",
        "input1Data = open('clean_real-Train.txt', 'r+')\n",
        "input2Data = open('clean_fake-Train.txt', 'r+')\n",
        "testData = open('test.txt')\n",
        "\n",
        "testDataForComparison = pd.read_csv('test.csv').iloc[:, 1:]\n",
        "\n",
        "input1List = []\n",
        "input2List = []\n",
        "testList = []\n",
        "\n",
        "read = input1Data.readline()\n",
        "read2 = input2Data.readline()\n",
        "read3 = testData.readline()\n",
        "rowCount = 0\n",
        "while read != \"\":\n",
        "    input1List.append(read)\n",
        "    read = input1Data.readline()\n",
        "    \n",
        "while read2 != \"\":\n",
        "    input2List.append(read2)\n",
        "    read2 = input2Data.readline()\n",
        "\n",
        "while read3 != \"\":\n",
        "    testList.append(read3)\n",
        "    read3 = testData.readline()\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AM0hwXewMn0W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Daha sonra iki farklı veri kümesinde bulunan 'real' ve 'fake' veri kümelerini birleştiriyoruz. Daha sonra LabelEncoder kullanarak Real olanlara 1, fake olanlara 0 veriyoruz."
      ]
    },
    {
      "metadata": {
        "id": "HtMraR2NhkjO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Real and Fake data combined.    \n",
        "liste = ['real' for i in range(len(input1List))]\n",
        "realInput = np.column_stack((input1List, liste))\n",
        "\n",
        "\n",
        "liste = ['fake' for i in range(len(input2List))]\n",
        "fakeInput = np.column_stack((input2List, liste))\n",
        "\n",
        "mainInput = realInput.tolist() + fakeInput.tolist()\n",
        "random.shuffle(mainInput)\n",
        "\n",
        "sentenceInput = []\n",
        "targetInput = []\n",
        "targetInputFloat = []\n",
        "for i in range(len(mainInput)):\n",
        "    sentenceInput.append(mainInput[i][0])\n",
        "    \n",
        "for i in range(len(mainInput)):\n",
        "    targetInput.append(mainInput[i][1])\n",
        "    \n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "lb = LabelEncoder()\n",
        "targetInputFloat = lb.fit_transform(targetInput)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3D3wvtrBNfFD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Veri Ön İşleme\n",
        "Bundan sonraki aşama veri ön işlemenin başladığı kısım. Veri kümesini bilgisayarın anlayabileceği sayılara çevirmeden önce bakıldığı zaman görülen bazı düzenlenmesi gereken yerler var. Bu yerlerin ne olduğundan bahsetmemiz ve en nihayetinde bunlardan kurtulmamız lazım. Yani veri kümemizi bazı veri ön işleme aşamalarından geçirmemiz gerekiyor. Bu aşamaları birkaç adımda sıralayabiliriz.\n",
        "1. Veri kümesi içerisinde noktalama işaretleri kelimelerin bir parçası olmadığı için veri kümesinden silinmelidir. İleride bahsedilecek ama biz temelinde kelimeleri 0 ve daha farklı sayılardan oluşan bir matris haline getirmeye çalışıyoruz. Burada eğer noktalama işaretlerini yok etmezsek bilgisayar bunları da bir sayı olarak düşünür ve gürültülü veriler ortaya çıkar. Bu istediğimiz bir durum değil.\n",
        "2. İlk harfi büyük olan  Mehmet ile ilk harfi küçük olan mehmet farklı anlamamı gelmeli? Tabiki hayır. Bu nedenle elimizde ki veri kümesinde büyük harfleri küçük harflere çevirerek bu ayrımı ortadan kaldırmış oluyoruz.\n",
        "3. Tıpkı büyük harf küçük harf durumunda olduğu gibi Ahmet’in ya da Ahmet’e gibi kelimelerin birbirinden farklılığı olmaması gerekir. Bu nedenle bir kelimenin başına veya sonuna gelen bütün eklerden kurtulmuş olması gerekir. Yani kelimelerin sadece köklerden oluşmasını isteriz.\n",
        "4. Birleştirici Kelimeler. Bu durumunda diğerlerinden hiçbir farkı yok. ‘ve’, ‘veya’, ‘ile’ gibi bağlaçların silinmesi gerekir. Burada bütün adımlarda yaptığımız temel amaç aynıdır. Bu adımların sonunda geriye sadece kelimelerin kökleri kalmalıdır. Bunun dışında hiçbir şey istemiyoruz. Şimdi gelin bu işlerin Python kodlamasında nasıl yapıldığına bir bakalım.\n"
      ]
    },
    {
      "metadata": {
        "id": "ZecTmv8Bh_G6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Stop Words\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Stemmer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "# Regular Expressions\n",
        "import re\n",
        "\n",
        "sentenceList = []\n",
        "for i in range(len(sentenceInput)):\n",
        "    # Regular Expressions\n",
        "    comment = re.sub('[^a-zA-Z]', ' ', sentenceInput[i]) #-> 1.Adım\n",
        "    # Upper Case Problem\n",
        "    comment = comment.lower() #-> 2. Adım\n",
        "    # Splitting\n",
        "    comment = comment.split() #-> 3.Adım\n",
        "    # Stemming and Stop Words\n",
        "    comment = [ps.stem(kelime) for kelime in comment if not kelime in set(stopwords.words('english'))] # -> 4. Adım\n",
        "    \n",
        "    comment = ' '.join(comment)\n",
        "    sentenceList.append(comment)\n",
        "    \n",
        "temporaryResult = pd.DataFrame(data=sentenceList)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "estPIRqf_ed4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Burada ilk 3 adım gerekli kütüphanelerin çağrıldığı kısımdır. Bizi asıl ilgilendiren ‘for’ döngüsünün başladığı kısım. Bu döngünün amacı her cümleye tek tek erişmektir. Yani bu döngü her çalıştığında biz bir satırda ki cümleye ulaşıyoruz. Dolayısıyla o cümleye ulaştığımıza göre her şeyi tek bir döngü içerisinde yapabiliriz.\n",
        "\n",
        "**1.Adım(Regular Expressions) ->** İlk aşamada düzenli ifadeler(Regular Expressions) denilen bir kavram ortaya çıkıyor. Bu kavram bilgisayar bilimlerinde çok ayrıntılı bir şekilde anlatılan ve esasında bir programlama dili oluştururken o dile kurallar verdiğimiz kısımda kendini gösteriyor. Burada tek bilmemiz gereken şey 2 büyük parantez içerisinde ki ifadede a-z bütün küçük harflerin, A-Z bütün büyük ifadelerin, tırnak işaretinin bütün küçük harflerin ve bütün büyük harflerin dışında kalan ifadeleri temsil ediyor olmasıdır. İkinci kısımda ki boşluk karakteri, bu ifadelerden herhangi biriyle karşılaşılır ise o ifadeyi boşluk karakteri ile yer değiştirecektir. Bunu yaptığımız zaman bütün noktalama işaretlerinden kurtulmuş oluyoruz.\n",
        "\n",
        "**2.Adım(Upper Case Problem) ->** Hatırlarsanız ikinci adımımız büyük harflerden kurtulmaktı. Burada da ikinci adımda lower() fonksiyonunu kullanarak bütün büyük harfleri küçük harfler ile yer değiştirmiş olduk.\n",
        "\n",
        "**3.Adım(Splitting) ->** Daha sonrasında yapmamız gereken şey kelime eklerinden ve bağlaç gibi cümleden bağımsız olan kelimelerden kurtulmak. Bunu yapmadan önce elimizde her bir kelimenin olduğu bir liste olması gerekiyor. Split() fonksiyonu cümledeki her bir kelimeyi bir listeye aktarıyor. Şimdi artık bu kelimelerin her birine tek tek ulaşarak o kelimenin üzerinde işlem yapabiliriz.\n",
        "\n",
        "**4.Adım(Stemming and Stop Words) ->** Bir alt satırda ki kod parçasında bir döngü ile her bir kelimeye tek tek ulaşıyor ve eğer bu kelime bir stopwords değilse o zaman stem() fonksiyonu ile başında veya sonunda bulunan eklerden kurtulmasını sağlıyoruz. Eğer bir stopwords ise o zaman kelimeyi tamamen yok ediyoruz.\n",
        "Bütün bu adımların sonunda artık elimizde o 4 adımda istediğimiz sadece köklerden oluşan bir veri kümesi olması gerekiyor. Bakalım gerçekten öyle olmuş mu?"
      ]
    },
    {
      "metadata": {
        "id": "dOebcgsp_pCM",
        "colab_type": "code",
        "outputId": "d2e3cacc-d33a-4e6c-e1fd-0601ca5acb41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1092
        }
      },
      "cell_type": "code",
      "source": [
        "print(temporaryResult)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                      0\n",
            "0         donald trump mexico border wall prototyp test\n",
            "1                    donald trump protest clash phoenix\n",
            "2             vision show donald trump socialis russian\n",
            "3                       trump visit israel western wall\n",
            "4     steve bannon keep giant whiteboard donald trum...\n",
            "5          georg bush vote hillari clinton donald trump\n",
            "6     donald trump prais australian coral sea th ann...\n",
            "7     hillari clinton brilliantli forc broke trump s...\n",
            "8     break sinc donald trump presid ford shift truc...\n",
            "9     democrat wore white donald trump big speech today\n",
            "10             presid donald trump first day white hous\n",
            "11               turnbul refus comment trump travel ban\n",
            "12                  trump protest wreak havoc us street\n",
            "13               trump predict brush year ago came true\n",
            "14         donald trump insist day trip golf club vacat\n",
            "15           key moment donald trump elect night speech\n",
            "16                        trump clinton part rig system\n",
            "17    donald trump hillari clinton trade jibe al smi...\n",
            "18                       trump hail passag gop tax bill\n",
            "19    could steal trump win next tuesday elector col...\n",
            "20     donald trump deni mock report blast meryl streep\n",
            "21             ukrain congratul trump god bless america\n",
            "22              donald trump bob corker jeff flake said\n",
            "23                     strategi could stop donald trump\n",
            "24                                        isra trumpess\n",
            "25     trump renew tough rhetor north korea talk missil\n",
            "26             suprem court nomine stake independ trump\n",
            "27    report dirti report blackmail montel help us h...\n",
            "28             la vega mass shoot evil donald trump say\n",
            "29              clown say world need laughter trump era\n",
            "...                                                 ...\n",
            "2747         trump announc move american embassi israel\n",
            "2748  donald trump new deal black america point abl ...\n",
            "2749  donald trump travel ban dealt anoth blow hawai...\n",
            "2750  trump clinton assembl armi lawyer contest elec...\n",
            "2751                 busi gaug fallout trump travel ban\n",
            "2752                      iran deal test trump independ\n",
            "2753   donald trump leav america younger sexier countri\n",
            "2754           donald trump access nuclear weapon scare\n",
            "2755                 donald trump vladimir putin meet g\n",
            "2756     donald trump whine isi cut head abl waterboard\n",
            "2757  obama tell black voter trump would toler kkk s...\n",
            "2758                 trump divid investor optim cautiou\n",
            "2759  win automak trump order fuel economi standard ...\n",
            "2760    donald trump close gap hillari clinton michigan\n",
            "2761  clinton camp claim media pro trump blame devas...\n",
            "2762            richard painter say donald trump doesnt\n",
            "2763  trump support scream press use anti semit slur...\n",
            "2764     black trump support need somebodi love america\n",
            "2765       trump pick rex tillerson grill committe hear\n",
            "2766  vorbild trump afd fordert ebenfal mauer grenz ...\n",
            "2767  trump team begin make list execut order eras o...\n",
            "2768        break judg issu restrain order donald trump\n",
            "2769  clinton email investig shift poll significantl...\n",
            "2770  brad pitt shock america tire liber take money ...\n",
            "2771  huckabe respond flag burn trump hater happen c...\n",
            "2772           riot america day donald trump win presid\n",
            "2773        trump g perform indic us declin world power\n",
            "2774            simpson react trump elect predict happi\n",
            "2775    hillari indict elect trump respond fbi investig\n",
            "2776                    last trump stamp foreign polici\n",
            "\n",
            "[2777 rows x 1 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "THi3jm8-PXaQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Evet, gerçekten veri kümesi içerisinde sadece köklerin olduğunu ve herhangi bir bağlaç kalmadığını görebiliyoruz. Tam olarak istediğimiz durum buydu. Artık veri kümesini bilgisayarın anlayabileceği 0 ve 1 lerden oluşan bir matris haline getirebiliriz.\n",
        "\n",
        "Bunun için kullandığımız Python kütüphanesi Scikit-Learn. Bu kütüphanenin içerisindeki CountVectorizer sınıfını kullanarak bu işlemi yapabiliyoruz. Bu sınıf veri kümesinde ki bütün kelimeleri alıp birer değişken haline getiriyor ve her bir cümlede o kelimenin olup olmadığına göre 0 ve 1 sayısını veriyor. En nihayetinde elimizde çok fazla sayıda 0 ve çok az sayıda 1’lerden oluşan bir sparse matris oluşuyor.\n"
      ]
    },
    {
      "metadata": {
        "id": "0NLkNMT1u-sX",
        "colab_type": "code",
        "outputId": "d3215798-5b29-451f-e88f-89e1a5e146e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2150
        }
      },
      "cell_type": "code",
      "source": [
        "# Vectorized All Data\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(sentenceList)\n",
        "\n",
        "result = pd.DataFrame(data=X.toarray())\n",
        "\n",
        "print(result)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      0     1     2     3     4     5     6     7     8     9     ...  3844  \\\n",
            "0        0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "1        0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "2        0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "3        0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "4        0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "5        0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "6        0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "7        0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "8        0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "9        0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "10       0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "11       0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "12       0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "13       0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "14       0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "15       0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "16       0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "17       0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "18       0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "19       0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "20       0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "21       0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "22       0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "23       0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "24       0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "25       0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "26       0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "27       0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "28       0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "29       0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
            "2747     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "2748     0     0     0     0     0     0     0     0     0     1  ...     0   \n",
            "2749     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "2750     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "2751     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "2752     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "2753     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "2754     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "2755     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "2756     0     0     0     0     0     0     0     0     0     1  ...     0   \n",
            "2757     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "2758     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "2759     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "2760     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "2761     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "2762     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "2763     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "2764     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "2765     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "2766     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "2767     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "2768     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "2769     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "2770     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "2771     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "2772     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "2773     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "2774     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "2775     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "2776     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
            "\n",
            "      3845  3846  3847  3848  3849  3850  3851  3852  3853  \n",
            "0        0     0     0     0     0     0     0     0     0  \n",
            "1        0     0     0     0     0     0     0     0     0  \n",
            "2        0     0     0     0     0     0     0     0     0  \n",
            "3        0     0     0     0     0     0     0     0     0  \n",
            "4        0     0     0     0     0     0     0     0     0  \n",
            "5        0     0     0     0     0     0     0     0     0  \n",
            "6        0     0     0     0     0     0     0     0     0  \n",
            "7        0     0     0     0     0     0     0     0     0  \n",
            "8        0     0     0     0     0     0     0     0     0  \n",
            "9        0     0     0     0     0     0     0     0     0  \n",
            "10       0     0     0     0     0     0     0     0     0  \n",
            "11       0     0     0     0     0     0     0     0     0  \n",
            "12       0     0     0     0     0     0     0     0     0  \n",
            "13       0     0     0     0     0     0     0     0     0  \n",
            "14       0     0     0     0     0     0     0     0     0  \n",
            "15       0     0     0     0     0     0     0     0     0  \n",
            "16       0     0     0     0     0     0     0     0     0  \n",
            "17       0     0     0     0     0     0     0     0     0  \n",
            "18       0     0     0     0     0     0     0     0     0  \n",
            "19       0     0     0     0     0     0     0     0     0  \n",
            "20       0     0     0     0     0     0     0     0     0  \n",
            "21       0     0     0     0     0     0     0     0     0  \n",
            "22       0     0     0     0     0     0     0     0     0  \n",
            "23       0     0     0     0     0     0     0     0     0  \n",
            "24       0     0     0     0     0     0     0     0     0  \n",
            "25       0     0     0     0     0     0     0     0     0  \n",
            "26       0     0     0     0     0     0     0     0     0  \n",
            "27       0     0     0     0     0     0     0     0     0  \n",
            "28       0     0     0     0     0     0     0     0     0  \n",
            "29       0     0     0     0     0     0     0     0     0  \n",
            "...    ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
            "2747     0     0     0     0     0     0     0     0     0  \n",
            "2748     0     0     0     0     0     0     0     0     0  \n",
            "2749     0     0     0     0     0     0     0     0     0  \n",
            "2750     0     0     0     0     0     0     0     0     0  \n",
            "2751     0     0     0     0     0     0     0     0     0  \n",
            "2752     0     0     0     0     0     0     0     0     0  \n",
            "2753     0     0     0     0     0     0     0     0     0  \n",
            "2754     0     0     0     0     0     0     0     0     0  \n",
            "2755     0     0     0     0     0     0     0     0     0  \n",
            "2756     0     0     0     0     0     0     0     0     0  \n",
            "2757     0     0     0     0     0     0     0     0     0  \n",
            "2758     0     0     0     0     0     0     0     0     0  \n",
            "2759     0     0     0     0     0     0     0     0     0  \n",
            "2760     0     0     0     0     0     0     0     0     0  \n",
            "2761     0     0     0     0     0     0     0     0     0  \n",
            "2762     0     0     0     0     0     0     0     0     0  \n",
            "2763     0     0     0     0     0     0     0     0     0  \n",
            "2764     0     0     0     0     0     0     0     0     0  \n",
            "2765     0     0     0     0     0     0     0     0     0  \n",
            "2766     0     0     0     0     0     0     1     0     0  \n",
            "2767     0     0     0     0     0     0     0     0     0  \n",
            "2768     0     0     0     0     0     0     0     0     0  \n",
            "2769     0     0     0     0     0     0     0     0     0  \n",
            "2770     0     0     0     0     0     0     0     0     0  \n",
            "2771     0     0     0     0     0     0     0     0     0  \n",
            "2772     0     0     0     0     0     0     0     0     0  \n",
            "2773     0     0     0     0     0     0     0     0     0  \n",
            "2774     0     0     0     0     0     0     0     0     0  \n",
            "2775     0     0     0     0     0     0     0     0     0  \n",
            "2776     0     0     0     0     0     0     0     0     0  \n",
            "\n",
            "[2777 rows x 3854 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zvDlR5bLQA6I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Şu noktadan itibaren bu veri kümesi artık bilgisayarın anlayabileceği bir model haline geldi."
      ]
    },
    {
      "metadata": {
        "id": "JxVOM_dEPzH5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Test veri kümemiz ile eğitim veri kümemiz birbirinden tamamen ayrı. Bu nedenle bu işlemlerin birebir aynısı test veri kümesi içinde uygulanmıştır.\n"
      ]
    },
    {
      "metadata": {
        "id": "XUGxSi7DdrQg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Test Data\n",
        "testSentenceList = []\n",
        "for i in range(len(testList)):\n",
        "    # Regular Expressions\n",
        "    comment = re.sub('[^a-zA-Z]', ' ', testList[i])\n",
        "    # Upper Case Problem\n",
        "    comment = comment.lower()\n",
        "    # Splitting\n",
        "    comment = comment.split()\n",
        "    # Stemming and Stop Words\n",
        "    comment = [ps.stem(kelime) for kelime in comment if not kelime in set(stopwords.words('english'))]\n",
        "    \n",
        "    comment = ' '.join(comment)\n",
        "    testSentenceList.append(comment)\n",
        "\n",
        "X = vectorizer.transform(testSentenceList)\n",
        "\n",
        "testResult = pd.DataFrame(data=X.toarray())\n",
        "# PCA for Test\n",
        "#testResult = pca.transform(testResult)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xrVkk9uuQnlj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Derin Öğrenme\n",
        "Nihayet artık veri kümesi bir model oluşturmaya hazır hale gelmiştir. Modelimiz bir derin öğrenme modelidir. Dolayısıyla derin öğrenmenin ne olduğundan kısaca bahsetmekte fayda var. Derin öğrenme, basit yapay sinir ağlarının daha karmaşık hale getirilmişi ve her bir katmana istediğimiz gibi müdahale edebildiğimiz bir yapısıdır. Karmaşıklık arttıkça derin öğrenmeye olan ihtiyaçta artıyor.\n"
      ]
    },
    {
      "metadata": {
        "id": "SmbmZpvrSKyX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "[![image](https://i.hizliresim.com/9a0AGO.jpg)](https://hizliresim.com/9a0AGO)"
      ]
    },
    {
      "metadata": {
        "id": "rGjMpTxjTayP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Şimdi nasıl bir model tasarladığıma kısaca bir bakalım. Elimizde 1 tane giriş katmanı, 1 tane saklı katman ve 2 tane seyreltme katmanı bulunmaktadır. Giriş ve saklı katman 1500’er tane nörona sahiptir. 2 tane seyreltme katmanımız var ve bu katmanlar 0.5 eşik değerine sahip. Bu seyreltme katmanlarının temel amacı ezberlemeyi azaltmaktır. Bu istemediğimiz bir durum. Dolayısıyla bu seyreltme katmanları eşik değerini 0.5 olarak girdiğimiz için burada 1500 tane olan nöron sayısını rasgele olarak yarıya düşürüyor. Bu noktadan sonra sonucu gerçek ya da yalan olarak belirlenecek olan tek bir nörona sahip bir tane çıkış katmanı oluşturuluyor. Daha geniş bir açıdan baktığımız zaman 1 tane giriş katmanımız, 1 tane saklı katmanımız ve 1 tanede çıkış katmanımız oluşuyor. Şimdi bunun nasıl kodlandığına bakalım.\n"
      ]
    },
    {
      "metadata": {
        "id": "YKks-ZQ9eCuv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras import optimizers\n",
        "import datetime\n",
        "\n",
        "time = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
        "\n",
        "kerasboard = TensorBoard(log_dir=\"/tmp/tensorboard/{}\".format(time))\n",
        "\n",
        "#result = result.reshape((2777, 1, 2000))\n",
        "#testResult = testResult.reshape((489, 1, 2000))\n",
        "\n",
        "classifier = Sequential()\n",
        "\n",
        "classifier.add(Dense(1500, kernel_initializer=\"glorot_normal\", activation='sigmoid', input_dim=3854)) #1 -> Giriş Katmanı\n",
        "#classifier.add(LSTM(1500, input_shape=(1, 2000), activation='sigmoid',return_sequences=False))\n",
        "classifier.add(Dropout(0.50))\n",
        "\n",
        "\n",
        "classifier.add(Dense(1500,kernel_initializer='glorot_normal' ,activation='sigmoid')) #2 -> Saklı Katman\n",
        "classifier.add(Dropout(0.50))\n",
        "\n",
        "\n",
        "classifier.add(Dense(1, kernel_initializer=\"glorot_normal\", activation='sigmoid')) #3 -> Çıkış Katmanı\n",
        "\n",
        "adam = optimizers.Adam(lr=0.0001, decay=0.00008)\n",
        "\n",
        "classifier.compile(optimizer=adam,loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c-nYSEmFB-Hs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Öncelikle kullanılan kütüphane Google’ın bir çalışanı tarafından tasarlanan Keras kütüphanesidir. Bu kütüphane arka planında Tensorflow denen başka bir kütüphaneyi kullanmaktadır. Bu kütüphaneyi ise Google şirketinin direk kendisi tasarladı. Bu iki kütüphane sayesinde Python üzerinden Derin öğrenme modelleri oluşturabiliyoruz. Bunlardan başka kütüphanelerde vardır fakat en çok kullanılan iki kütüphane bu ikisidir.\n",
        "\n",
        "Burada odaklanmamız gereken kısımlar Dense ve Dropout fonksiyonlarının olduğu kısımlardır. \n",
        "\n",
        "**Giriş Katmanı -> ** 1 numaralı kısımda ilk parametre olarak 1500 sayısını girerek nöron sayısını girmiş olduk. \n",
        "\n",
        "İkinci parametre kernel_initializer başlangıçta hangi istatistiksel dağılımı kullanarak nöronlara ağırlıklar vereceğimizi belirlediğimiz kısım. Ağırlık verirken dikkat edilmesi gereken 3 tane husus vardır.\n",
        "1. Her bir optimizasyon gerçekleştiğinde ağırlıkların ortalamasının sıfıra eşit olması gerekir.\n",
        "2. Bütün ağırlıkların varyanslarının eşit olması gerekmektedir yani değişen varyanslılık sorununun olmamasıdır.\n",
        "3. Ağırlıkların kesinlikle sıfıra eşit olmaması ve ne çok küçük değerler ne de çok büyük değerlere eşit olmasıdır.\n",
        "\n",
        "Bütün bu hususların ışığında en çok kullanılan 2 tane initializer Xavier ve He'dir. Ben çalışmamda Xavier(glorot_normal) kullandım.\n",
        "\n",
        "Diğer bir parametre aktivasyon fonksiyonudur, bu fonksiyon girişteki katsayıyı alıp kendi fonksiyonunda işlem uygulayarak çıktı katsayısı elde etmemizi sağlıyor. Daha sonra bu çıktı katsayısı ya direk sonucun kendisi olur ya da bir başka katmanın girdisi olur.\n",
        "Aktivasyon fonksiyonlarının 2 tane ana özelliği vardır.\n",
        "1. Derin Öğrenme kullanırken elimizdeki veri setinin karmaşıklığı o veri setinin çok büyük bir ihtimalle doğrusal olmayacağını ön görür. Bu sebeple bu fonksiyonlar doğrusal olmayan fonksiyonlardır.\n",
        "2. Her bir geri beslemede bu aktivasyon fonksiyonlarının türevi alınarak en optimal sonuca ulaşmak amaçlanır. Bu nedenle kolay türevi alınabilen fonksiyonlar tercih edilir böylece zaten sistemi çok  yoran derin öğrenme algoritmalarının iş yükü biraz daha hafifletilmiş olur.\n",
        "\n",
        "Burada aktivasyon fonksiyonu olarak en popüleri olan **Sigmoid** fonksiyonu kullanmayı tercih ettim.\n",
        "\n",
        "En son parametre olarakta elimizdeki veri kümesinin değişken sayısını vermemiz gerekiyor. Bu bilgi veri kümesindeki kelime sayısına eşittir yani 3854.\n",
        "\n",
        "Daha sonra Dropout katmanını oluşturuyoruz ve eşik değeri olarak 0.5 giriyoruz. \n",
        "\n",
        "**Saklı Katman -> ** 2 numaralı kısımda 1 numaralı kısımda yaptığımız şeyden farklı olan hiçbir şey yapmıyoruz. \n",
        "\n",
        "**Çıkış Katmanı -> ** 3 numaralı kısımda ise sonucu doğru ya da yalan olan 1 tane nörona sahip bir katman oluşturuyoruz.\n",
        "\n",
        "Bundan sonraki aşamada bir optimizasyon fonksiyonu belirlememiz gerekiyor. Bu fonksiyonun amacı ise her bir geri beslemede nöronların katsayılarını daha optimal hale getirip doğruluk yüzdemizi arttırmayı amaçlamak. Bakalım kodlarımız nasıl çalışmış."
      ]
    },
    {
      "metadata": {
        "id": "3EX_pA1KCJoD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1448
        },
        "outputId": "baa9a5cc-71e1-4ca5-cb08-83d1b55ae946"
      },
      "cell_type": "code",
      "source": [
        "classifier.fit(result, targetInputFloat, epochs=40, callbacks=[kerasboard])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.7504 - acc: 0.5481\n",
            "Epoch 2/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.7314 - acc: 0.5524\n",
            "Epoch 3/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.7235 - acc: 0.5614\n",
            "Epoch 4/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.7209 - acc: 0.5517\n",
            "Epoch 5/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.6952 - acc: 0.5715\n",
            "Epoch 6/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.6779 - acc: 0.5920\n",
            "Epoch 7/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.6590 - acc: 0.6118\n",
            "Epoch 8/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.6421 - acc: 0.6323\n",
            "Epoch 9/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.6329 - acc: 0.6421\n",
            "Epoch 10/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.6131 - acc: 0.6601\n",
            "Epoch 11/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.5993 - acc: 0.6759\n",
            "Epoch 12/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.5763 - acc: 0.6986\n",
            "Epoch 13/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.5510 - acc: 0.7213\n",
            "Epoch 14/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.5385 - acc: 0.7303\n",
            "Epoch 15/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.5077 - acc: 0.7501\n",
            "Epoch 16/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.4935 - acc: 0.7605\n",
            "Epoch 17/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.4617 - acc: 0.7857\n",
            "Epoch 18/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.4443 - acc: 0.7965\n",
            "Epoch 19/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.4394 - acc: 0.7947\n",
            "Epoch 20/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.4138 - acc: 0.8163\n",
            "Epoch 21/40\n",
            "2777/2777 [==============================] - 14s 5ms/step - loss: 0.4046 - acc: 0.8113\n",
            "Epoch 22/40\n",
            "2777/2777 [==============================] - 14s 5ms/step - loss: 0.4113 - acc: 0.8099\n",
            "Epoch 23/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.3768 - acc: 0.8279\n",
            "Epoch 24/40\n",
            "2777/2777 [==============================] - 14s 5ms/step - loss: 0.3654 - acc: 0.8383\n",
            "Epoch 25/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.3466 - acc: 0.8516\n",
            "Epoch 26/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.3597 - acc: 0.8351\n",
            "Epoch 27/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.3240 - acc: 0.8578\n",
            "Epoch 28/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.3467 - acc: 0.8524\n",
            "Epoch 29/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.3215 - acc: 0.8552\n",
            "Epoch 30/40\n",
            "2777/2777 [==============================] - 14s 5ms/step - loss: 0.3177 - acc: 0.8614\n",
            "Epoch 31/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.3095 - acc: 0.8621\n",
            "Epoch 32/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.2896 - acc: 0.8711\n",
            "Epoch 33/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.3017 - acc: 0.8718\n",
            "Epoch 34/40\n",
            "2777/2777 [==============================] - 14s 5ms/step - loss: 0.2846 - acc: 0.8754\n",
            "Epoch 35/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.2888 - acc: 0.8783\n",
            "Epoch 36/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.2798 - acc: 0.8750\n",
            "Epoch 37/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.2722 - acc: 0.8840\n",
            "Epoch 38/40\n",
            "2777/2777 [==============================] - 14s 5ms/step - loss: 0.2752 - acc: 0.8783\n",
            "Epoch 39/40\n",
            "2777/2777 [==============================] - 14s 5ms/step - loss: 0.2595 - acc: 0.8905\n",
            "Epoch 40/40\n",
            "2777/2777 [==============================] - 13s 5ms/step - loss: 0.2686 - acc: 0.8740\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0413c52240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "IaaQpqwzKcCL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Modellemede toplamda 40 tane geri besleme yani optimizasyon çalışması yapıldı. Loss yani hata değerlerinin giderek düşmesini bekliyoruz gerçekten de öyle olmuş. Accuracy yani doğruluk yüzdelerinin de her bir geri beslemede artmasını amaçlıyoruz. Oda artmış, ilk başta %52 doğru tahmin yapabilirken 40. geri beslemede %89 oranına çıkmış. Temel olarak doğruluk yüzdesinin bir noktadan sonra artmamasını isteriz. Bu durum öğrenme hızımızı ne kadar iyi belirlediğimize bağlı olarak değişir. \n",
        "\n",
        "Bu oranların hepsi eğitim veri kümesi için, bakalım modele test veri kümesini verdiğimizde nasıl bir sonuç alacağız en nihayetinde bizim için asıl önemli olan kısım modelin daha önce bilmediği bir cümleyi nasıl analiz ettiği.\n"
      ]
    },
    {
      "metadata": {
        "id": "lUmHwukhKvC1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "a3deedf5-7846-439e-90cc-624d45736bf9"
      },
      "cell_type": "code",
      "source": [
        "prediction = classifier.predict(testResult)\n",
        "\n",
        "prediction = (prediction > 0.5)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "acScoreNN = accuracy_score(lb.fit_transform(testDataForComparison), prediction)\n",
        "print(\"Accuracy : \" + str(acScoreNN))\n",
        "\n",
        "probaNN = classifier.predict_proba(testResult)\n",
        "print(\"NN AUC \" + str(metrics.roc_auc_score(lb.fit_transform(testDataForComparison), probaNN)))\n",
        "\n",
        "print(\"tensorboard --logdir=\"+kerasboard.log_dir)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy : 0.8282208588957055\n",
            "NN AUC 0.9124759741394374\n",
            "tensorboard --logdir=/tmp/tensorboard/2019_05_02_12_47_31\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "u3ty-SzJK3uu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Sonuçlarda görüldüğü gibi model daha önce bilmediği bir veri kümesiyle karşılaştığı zaman %83 doğru tahmin yüzdesine ulaşmış.\n",
        "\n",
        "ROC Eğrisi altında kalan alan %91 çıkmış. Bundan çıkan yorum modelin %91 oranında doğruyla yanlışı ayırt edebilme gücüne sahip olduğudur. Bu değer 1’e ne kadar yakın olursa model o kadar iyi demektir. Tabi ROC Eğrisi sonucunun doğruluk yüzdesine göre daha yanıltıcı sonuç verebileceğini söylemekte fayda var.\n"
      ]
    },
    {
      "metadata": {
        "id": "yQ7r6NiSNA2q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Şimdi oluşturmuş olduğumuz derin öğrenme modelinin her bir geri beslemede doğruluk yüzdesini nasıl beslediğini TensorBoard ortamında görselleştirelim."
      ]
    },
    {
      "metadata": {
        "id": "PEt5LISjPRy8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard.notebook\n",
        "\n",
        "#Activate Tensorboard\n",
        "%tensorboard --logdir=/tmp/tensorboard/2019_05_02_12_47_31"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9pWrToPbNi18",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TensorBoard'u çağırmak için derin öğrenme modelini kurarken oluşturduğumuz log adresini kullanıyoruz. Normal şartlarda bu log adresinin farklı bir tarayıcı sekmesinde açılmasını bekleriz fakat Jupyter Notebook'a yeni gelmiş bir eklenti sayesinde bunu artık kod hücresinin içerisinde açabiliyoruz. (tensorboard.notebook eklentisi)"
      ]
    },
    {
      "metadata": {
        "id": "yYCRmiG1OnUS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Bundan sonraki aşamada oluşturmuş olduğumuz derin öğrenme modelini Pickle kütüphanesini kullanarak kaydediyoruz. Böylece ne zaman istersek model üzerinden tahminler gerçekleştirebileceğiz."
      ]
    },
    {
      "metadata": {
        "id": "r1wi9YUKjftV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Saving the best model with Pickle (Neural %83)\n",
        "import pickle\n",
        "pickle.dump(classifier, open(\"NeuralNews\", 'wb'))\n",
        "\n",
        "loading = pickle.load(open(\"NeuralNews\", 'rb'))\n",
        "predictionPickleNeural = loading.predict(testResult)\n",
        "\n",
        "predictionPickleNeural = (predictionPickleNeural > 0.5)\n",
        "\n",
        "acScorePickleNeural = accuracy_score(lb.fit_transform(testDataForComparison), predictionPickleNeural)\n",
        "print(\"Accuracy Pickle Neural : \" + str(acScorePickleNeural))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}